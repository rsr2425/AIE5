{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research Unrolled\n",
    "\n",
    "In this notebook, we'll look at the unrolled version of the Open Deep Research graph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "![image](https://i.imgur.com/YzbY9vJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies: \n",
    "\n",
    "You'll need two different API keys for the default method - though you can customize this graph to your heart's content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: State\n",
    "\n",
    "The state structure organizes all the information needed to generate and refine a report on a given topic through multiple steps. Here’s a breakdown:\n",
    "\n",
    "- **Report-Level State:**  \n",
    "  - **Topic:** The overall subject of the report.  \n",
    "  - **Feedback on Report Plan:** Comments or evaluations on the preliminary report outline.  \n",
    "  - **Sections:** A list of individual report sections (each with its own name, description, whether research is needed, and content).  \n",
    "  - **Completed Sections:** A collection of sections that have been fully developed, which is also used by downstream processes (e.g., a Send() API).  \n",
    "  - **Research-Based Content:** A string that gathers content produced from web research, which may be integrated into the final report.  \n",
    "  - **Final Report:** The completed report after all sections and revisions are combined.\n",
    "\n",
    "- **Section-Level State:**  \n",
    "  For each report section, the state captures:  \n",
    "  - **Section Details:** Including the section’s name, a brief overview (description), a flag indicating if web research should be performed, and the actual content.  \n",
    "  - **Search Process:** The number of search iterations that have been executed and a list of search queries used to gather information.  \n",
    "  - **Source Content:** A formatted string containing the relevant material obtained from web searches.  \n",
    "  - **Integration:** Both the research-derived content and the list of fully completed sections are tracked to ensure they can be merged into the overall report.\n",
    "\n",
    "- **Search Queries and Feedback:**  \n",
    "  - **Search Queries:** Represented as individual query objects, these encapsulate the strings used to perform web searches.  \n",
    "  - **Feedback:** After generating parts of the report, feedback is provided as a grade (either \"pass\" or \"fail\") along with any follow-up queries to improve the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "    research: bool = Field(\n",
    "        description=\"Whether to perform web research for this section of the report.\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"The content of the section.\"\n",
    "    )   \n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query for web search.\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[SearchQuery] = Field(\n",
    "        description=\"List of search queries.\",\n",
    "    )\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"pass\",\"fail\"] = Field(\n",
    "        description=\"Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail').\"\n",
    "    )\n",
    "    follow_up_queries: List[SearchQuery] = Field(\n",
    "        description=\"List of follow-up search queries.\",\n",
    "    )\n",
    "\n",
    "class ReportStateInput(TypedDict):\n",
    "    topic: str # Report topic\n",
    "    \n",
    "class ReportStateOutput(TypedDict):\n",
    "    final_report: str # Final report\n",
    "\n",
    "class ReportState(TypedDict):\n",
    "    topic: str # Report topic    \n",
    "    feedback_on_report_plan: str # Feedback on the report plan\n",
    "    sections: list[Section] # List of report sections \n",
    "    completed_sections: Annotated[list, operator.add] # Send() API key\n",
    "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
    "    final_report: str # Final report\n",
    "\n",
    "class SectionState(TypedDict):\n",
    "    topic: str # Report topic\n",
    "    section: Section # Report section  \n",
    "    search_iterations: int # Number of search iterations done\n",
    "    search_queries: list[SearchQuery] # List of search queries\n",
    "    source_str: str # String of formatted source content from web search\n",
    "    report_sections_from_research: str # String of any completed sections from research to write final sections\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SectionOutputState(TypedDict):\n",
    "    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Utilities and Helpers\n",
    "\n",
    "We have a number of utility functions that we'll use to build our graph. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import requests\n",
    "\n",
    "from tavily import TavilyClient, AsyncTavilyClient\n",
    "from langchain_community.retrievers import ArxivRetriever\n",
    "from langchain_community.utilities.pubmed import PubMedAPIWrapper\n",
    "from exa_py import Exa\n",
    "from typing import List, Optional, Dict, Any\n",
    "from langsmith import traceable\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "tavily_async_client = AsyncTavilyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a small helper that makes sure your configuration values are in a consistent format. If you pass in a string, it simply returns that string. However, if you pass in an enum (a special type of value), it will extract and return the underlying value of that enum. This helps when your configuration might come in different types but you need to work with strings consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_value(value):\n",
    "    \"\"\"\n",
    "    Helper function to handle both string and enum cases of configuration values\n",
    "    \"\"\"\n",
    "    return value if isinstance(value, str) else value.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function filters a configuration dictionary so that it only includes parameters that are allowed for a specific search API. It works by looking up a list of accepted parameter names for the given API (like “exa” or “pubmed”) and then stripping out any other entries from the configuration. If no configuration is provided, it simply returns an empty dictionary. This ensures that only valid and expected parameters are sent to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get search parameters based on the search API and config\n",
    "def get_search_params(search_api: str, search_api_config: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Filters the search_api_config dictionary to include only parameters accepted by the specified search API.\n",
    "\n",
    "    Args:\n",
    "        search_api (str): The search API identifier (e.g., \"exa\", \"tavily\").\n",
    "        search_api_config (Optional[Dict[str, Any]]): The configuration dictionary for the search API.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary of parameters to pass to the search function.\n",
    "    \"\"\"\n",
    "    # Define accepted parameters for each search API\n",
    "    SEARCH_API_PARAMS = {\n",
    "        \"exa\": [\"max_characters\", \"num_results\", \"include_domains\", \"exclude_domains\", \"subpages\"],\n",
    "        \"tavily\": [],  # Tavily currently accepts no additional parameters\n",
    "        \"perplexity\": [],  # Perplexity accepts no additional parameters\n",
    "        \"arxiv\": [\"load_max_docs\", \"get_full_documents\", \"load_all_available_meta\"],\n",
    "        \"pubmed\": [\"top_k_results\", \"email\", \"api_key\", \"doc_content_chars_max\"],\n",
    "    }\n",
    "\n",
    "    # Get the list of accepted parameters for the given search API\n",
    "    accepted_params = SEARCH_API_PARAMS.get(search_api, [])\n",
    "\n",
    "    # If no config provided, return an empty dict\n",
    "    if not search_api_config:\n",
    "        return {}\n",
    "\n",
    "    # Filter the config to only include accepted parameters\n",
    "    return {k: v for k, v in search_api_config.items() if k in accepted_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a collection of search results—possibly from several responses—and formats them into a neat, human-readable string. It starts by combining all the results and then removes any duplicates (using the URL to check for repeats). For each unique source, it prints out the title, URL, and a summary of the content. If enabled, it also includes a trimmed version of the full source content, ensuring that the text does not exceed a specified token limit. This results in a clean, consolidated overview of your search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
    "    \"\"\"\n",
    "    Takes a list of search responses and formats them into a readable string.\n",
    "    Limits the raw_content to approximately max_tokens_per_source.\n",
    " \n",
    "    Args:\n",
    "        search_responses: List of search response dicts, each containing:\n",
    "            - query: str\n",
    "            - results: List of dicts with fields:\n",
    "                - title: str\n",
    "                - url: str\n",
    "                - content: str\n",
    "                - score: float\n",
    "                - raw_content: str|None\n",
    "        max_tokens_per_source: int\n",
    "        include_raw_content: bool\n",
    "            \n",
    "    Returns:\n",
    "        str: Formatted string with deduplicated sources\n",
    "    \"\"\"\n",
    "     # Collect all results\n",
    "    sources_list = []\n",
    "    for response in search_response:\n",
    "        sources_list.extend(response['results'])\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {source['url']: source for source in sources_list}\n",
    "\n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            # Using rough estimate of 4 characters per token\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            # Handle None raw_content\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "                \n",
    "    return formatted_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed to help organize documentation or reports, this function takes a list of section objects and turns them into a well-formatted string. For each section, it prints a header with the section number and name, followed by its description, any research notes, and the main content (or a placeholder if the content isn’t written yet). The output is clearly separated by visual dividers, making it easy to read and understand the structure of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sections(sections: list[Section]) -> str:\n",
    "    \"\"\" Format a list of sections into a string \"\"\"\n",
    "    formatted_str = \"\"\n",
    "    for idx, section in enumerate(sections, 1):\n",
    "        formatted_str += f\"\"\"\n",
    "{'='*60}\n",
    "Section {idx}: {section.name}\n",
    "{'='*60}\n",
    "Description:\n",
    "{section.description}\n",
    "Requires Research: \n",
    "{section.research}\n",
    "\n",
    "Content:\n",
    "{section.content if section.content else '[Not yet written]'}\n",
    "\n",
    "\"\"\"\n",
    "    return formatted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs multiple web searches concurrently using the Tavily API. You simply provide it with a list of search queries, and it creates asynchronous tasks for each one. The function then gathers all the responses together, each containing details like the title, URL, snippet of content, and optionally the raw content. This is particularly useful when you need to search several queries at once without waiting for each one to finish sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def tavily_search_async(search_queries):\n",
    "    \"\"\"\n",
    "    Performs concurrent web searches using the Tavily API.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[SearchQuery]): List of search queries to process\n",
    "\n",
    "    Returns:\n",
    "            List[dict]: List of search responses from Tavily API, one per query. Each response has format:\n",
    "                {\n",
    "                    'query': str, # The original search query\n",
    "                    'follow_up_questions': None,      \n",
    "                    'answer': None,\n",
    "                    'images': list,\n",
    "                    'results': [                     # List of search results\n",
    "                        {\n",
    "                            'title': str,            # Title of the webpage\n",
    "                            'url': str,              # URL of the result\n",
    "                            'content': str,          # Summary/snippet of content\n",
    "                            'score': float,          # Relevance score\n",
    "                            'raw_content': str|None  # Full page content if available\n",
    "                        },\n",
    "                        ...\n",
    "                    ]\n",
    "                }\n",
    "    \"\"\"\n",
    "    \n",
    "    search_tasks = []\n",
    "    for query in search_queries:\n",
    "            search_tasks.append(\n",
    "                tavily_async_client.search(\n",
    "                    query,\n",
    "                    max_results=5,\n",
    "                    include_raw_content=True,\n",
    "                    topic=\"general\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Execute all searches concurrently\n",
    "    search_docs = await asyncio.gather(*search_tasks)\n",
    "\n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function leverages the Perplexity API to perform web searches. For every query provided, it sends a request with a prompt to fetch factual content. The response is parsed to extract the main answer and any associated citations. The first citation contains the full content, while any additional citations serve as secondary sources. The results are then packaged into a standardized structure, making it clear which source provides the complete information and which ones are supporting references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def perplexity_search(search_queries):\n",
    "    \"\"\"Search the web using the Perplexity API.\n",
    "    \n",
    "    Args:\n",
    "        search_queries (List[SearchQuery]): List of search queries to process\n",
    "  \n",
    "    Returns:\n",
    "        List[dict]: List of search responses from Perplexity API, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': list,\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the search result\n",
    "                        'url': str,              # URL of the result\n",
    "                        'content': str,          # Summary/snippet of content\n",
    "                        'score': float,          # Relevance score\n",
    "                        'raw_content': str|None  # Full content or None for secondary citations\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('PERPLEXITY_API_KEY')}\"\n",
    "    }\n",
    "    \n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "\n",
    "        payload = {\n",
    "            \"model\": \"sonar-pro\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Search the web and provide factual information with sources.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://api.perplexity.ai/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # Parse the response\n",
    "        data = response.json()\n",
    "        content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        citations = data.get(\"citations\", [\"https://perplexity.ai\"])\n",
    "        \n",
    "        # Create results list for this query\n",
    "        results = []\n",
    "        \n",
    "        # First citation gets the full content\n",
    "        results.append({\n",
    "            \"title\": f\"Perplexity Search, Source 1\",\n",
    "            \"url\": citations[0],\n",
    "            \"content\": content,\n",
    "            \"raw_content\": content,\n",
    "            \"score\": 1.0  # Adding score to match Tavily format\n",
    "        })\n",
    "        \n",
    "        # Add additional citations without duplicating content\n",
    "        for i, citation in enumerate(citations[1:], start=2):\n",
    "            results.append({\n",
    "                \"title\": f\"Perplexity Search, Source {i}\",\n",
    "                \"url\": citation,\n",
    "                \"content\": \"See primary source for full content\",\n",
    "                \"raw_content\": None,\n",
    "                \"score\": 0.5  # Lower score for secondary sources\n",
    "            })\n",
    "        \n",
    "        # Format response to match Tavily structure\n",
    "        search_docs.append({\n",
    "            \"query\": query,\n",
    "            \"follow_up_questions\": None,\n",
    "            \"answer\": None,\n",
    "            \"images\": [],\n",
    "            \"results\": results\n",
    "        })\n",
    "    \n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Exa API, this function performs web searches with a lot of flexibility. It allows you to limit the number of characters retrieved for each result, specify how many results you want, and even control which domains to include or exclude. If you need more detail, it can also fetch additional subpages related to each result. The function processes each query (while handling rate limits), deduplicates the results by their URL, and combines elements like summaries and text into a single, formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def exa_search(search_queries, max_characters: Optional[int] = None, num_results=5, \n",
    "                     include_domains: Optional[List[str]] = None, \n",
    "                     exclude_domains: Optional[List[str]] = None,\n",
    "                     subpages: Optional[int] = None):\n",
    "    \"\"\"Search the web using the Exa API.\n",
    "    \n",
    "    Args:\n",
    "        search_queries (List[SearchQuery]): List of search queries to process\n",
    "        max_characters (int, optional): Maximum number of characters to retrieve for each result's raw content.\n",
    "                                       If None, the text parameter will be set to True instead of an object.\n",
    "        num_results (int): Number of search results per query. Defaults to 5.\n",
    "        include_domains (List[str], optional): List of domains to include in search results. \n",
    "            When specified, only results from these domains will be returned.\n",
    "        exclude_domains (List[str], optional): List of domains to exclude from search results.\n",
    "            Cannot be used together with include_domains.\n",
    "        subpages (int, optional): Number of subpages to retrieve per result. If None, subpages are not retrieved.\n",
    "        \n",
    "    Returns:\n",
    "        List[dict]: List of search responses from Exa API, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': list,\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the search result\n",
    "                        'url': str,              # URL of the result\n",
    "                        'content': str,          # Summary/snippet of content\n",
    "                        'score': float,          # Relevance score\n",
    "                        'raw_content': str|None  # Full content or None for secondary citations\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    # Check that include_domains and exclude_domains are not both specified\n",
    "    if include_domains and exclude_domains:\n",
    "        raise ValueError(\"Cannot specify both include_domains and exclude_domains\")\n",
    "    \n",
    "    # Initialize Exa client (API key should be configured in your .env file)\n",
    "    exa = Exa(api_key = f\"{os.getenv('EXA_API_KEY')}\")\n",
    "    \n",
    "    # Define the function to process a single query\n",
    "    async def process_query(query):\n",
    "        # Use run_in_executor to make the synchronous exa call in a non-blocking way\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Define the function for the executor with all parameters\n",
    "        def exa_search_fn():\n",
    "            # Build parameters dictionary\n",
    "            kwargs = {\n",
    "                # Set text to True if max_characters is None, otherwise use an object with max_characters\n",
    "                \"text\": True if max_characters is None else {\"max_characters\": max_characters},\n",
    "                \"summary\": True,  # This is an amazing feature by EXA. It provides an AI generated summary of the content based on the query\n",
    "                \"num_results\": num_results\n",
    "            }\n",
    "            \n",
    "            # Add optional parameters only if they are provided\n",
    "            if subpages is not None:\n",
    "                kwargs[\"subpages\"] = subpages\n",
    "                \n",
    "            if include_domains:\n",
    "                kwargs[\"include_domains\"] = include_domains\n",
    "            elif exclude_domains:\n",
    "                kwargs[\"exclude_domains\"] = exclude_domains\n",
    "                \n",
    "            return exa.search_and_contents(query, **kwargs)\n",
    "        \n",
    "        response = await loop.run_in_executor(None, exa_search_fn)\n",
    "        \n",
    "        # Format the response to match the expected output structure\n",
    "        formatted_results = []\n",
    "        seen_urls = set()  # Track URLs to avoid duplicates\n",
    "        \n",
    "        # Helper function to safely get value regardless of if item is dict or object\n",
    "        def get_value(item, key, default=None):\n",
    "            if isinstance(item, dict):\n",
    "                return item.get(key, default)\n",
    "            else:\n",
    "                return getattr(item, key, default) if hasattr(item, key) else default\n",
    "        \n",
    "        # Access the results from the SearchResponse object\n",
    "        results_list = get_value(response, 'results', [])\n",
    "        \n",
    "        # First process all main results\n",
    "        for result in results_list:\n",
    "            # Get the score with a default of 0.0 if it's None or not present\n",
    "            score = get_value(result, 'score', 0.0)\n",
    "            \n",
    "            # Combine summary and text for content if both are available\n",
    "            text_content = get_value(result, 'text', '')\n",
    "            summary_content = get_value(result, 'summary', '')\n",
    "            \n",
    "            content = text_content\n",
    "            if summary_content:\n",
    "                if content:\n",
    "                    content = f\"{summary_content}\\n\\n{content}\"\n",
    "                else:\n",
    "                    content = summary_content\n",
    "            \n",
    "            title = get_value(result, 'title', '')\n",
    "            url = get_value(result, 'url', '')\n",
    "            \n",
    "            # Skip if we've seen this URL before (removes duplicate entries)\n",
    "            if url in seen_urls:\n",
    "                continue\n",
    "                \n",
    "            seen_urls.add(url)\n",
    "            \n",
    "            # Main result entry\n",
    "            result_entry = {\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"content\": content,\n",
    "                \"score\": score,\n",
    "                \"raw_content\": text_content\n",
    "            }\n",
    "            \n",
    "            # Add the main result to the formatted results\n",
    "            formatted_results.append(result_entry)\n",
    "        \n",
    "        # Now process subpages only if the subpages parameter was provided\n",
    "        if subpages is not None:\n",
    "            for result in results_list:\n",
    "                subpages_list = get_value(result, 'subpages', [])\n",
    "                for subpage in subpages_list:\n",
    "                    # Get subpage score\n",
    "                    subpage_score = get_value(subpage, 'score', 0.0)\n",
    "                    \n",
    "                    # Combine summary and text for subpage content\n",
    "                    subpage_text = get_value(subpage, 'text', '')\n",
    "                    subpage_summary = get_value(subpage, 'summary', '')\n",
    "                    \n",
    "                    subpage_content = subpage_text\n",
    "                    if subpage_summary:\n",
    "                        if subpage_content:\n",
    "                            subpage_content = f\"{subpage_summary}\\n\\n{subpage_content}\"\n",
    "                        else:\n",
    "                            subpage_content = subpage_summary\n",
    "                    \n",
    "                    subpage_url = get_value(subpage, 'url', '')\n",
    "                    \n",
    "                    # Skip if we've seen this URL before\n",
    "                    if subpage_url in seen_urls:\n",
    "                        continue\n",
    "                        \n",
    "                    seen_urls.add(subpage_url)\n",
    "                    \n",
    "                    formatted_results.append({\n",
    "                        \"title\": get_value(subpage, 'title', ''),\n",
    "                        \"url\": subpage_url,\n",
    "                        \"content\": subpage_content,\n",
    "                        \"score\": subpage_score,\n",
    "                        \"raw_content\": subpage_text\n",
    "                    })\n",
    "        \n",
    "        # Collect images if available (only from main results to avoid duplication)\n",
    "        images = []\n",
    "        for result in results_list:\n",
    "            image = get_value(result, 'image')\n",
    "            if image and image not in images:  # Avoid duplicate images\n",
    "                images.append(image)\n",
    "                \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"follow_up_questions\": None,\n",
    "            \"answer\": None,\n",
    "            \"images\": images,\n",
    "            \"results\": formatted_results\n",
    "        }\n",
    "    \n",
    "    # Process all queries sequentially with delay to respect rate limit\n",
    "    search_docs = []\n",
    "    for i, query in enumerate(search_queries):\n",
    "        try:\n",
    "            # Add delay between requests (0.25s = 4 requests per second, well within the 5/s limit)\n",
    "            if i > 0:  # Don't delay the first request\n",
    "                await asyncio.sleep(0.25)\n",
    "            \n",
    "            result = await process_query(query)\n",
    "            search_docs.append(result)\n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            print(f\"Error processing query '{query}': {str(e)}\")\n",
    "            # Add a placeholder result for failed queries to maintain index alignment\n",
    "            search_docs.append({\n",
    "                \"query\": query,\n",
    "                \"follow_up_questions\": None,\n",
    "                \"answer\": None,\n",
    "                \"images\": [],\n",
    "                \"results\": [],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            \n",
    "            # Add additional delay if we hit a rate limit error\n",
    "            if \"429\" in str(e):\n",
    "                print(\"Rate limit exceeded. Adding additional delay...\")\n",
    "                await asyncio.sleep(1.0)  # Add a longer delay if we hit a rate limit\n",
    "    \n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is tailored for searching academic papers on arXiv. It runs asynchronously, so it can handle multiple queries without blocking. For each search query, it initializes an arXiv retriever that gathers documents along with their metadata (like authors, publication dates, and summaries). The results are then formatted to include useful details such as a link to the paper and even a link to the PDF if available. It also assigns a relevance score to each paper and respects arXiv’s rate limits by adding delays between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def arxiv_search_async(search_queries, load_max_docs=5, get_full_documents=True, load_all_available_meta=True):\n",
    "    \"\"\"\n",
    "    Performs concurrent searches on arXiv using the ArxivRetriever.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries or article IDs\n",
    "        load_max_docs (int, optional): Maximum number of documents to return per query. Default is 5.\n",
    "        get_full_documents (bool, optional): Whether to fetch full text of documents. Default is True.\n",
    "        load_all_available_meta (bool, optional): Whether to load all available metadata. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of search responses from arXiv, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the paper\n",
    "                        'url': str,              # URL (Entry ID) of the paper\n",
    "                        'content': str,          # Formatted summary with metadata\n",
    "                        'score': float,          # Relevance score (approximated)\n",
    "                        'raw_content': str|None  # Full paper content if available\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def process_single_query(query):\n",
    "        try:\n",
    "            # Create retriever for each query\n",
    "            retriever = ArxivRetriever(\n",
    "                load_max_docs=load_max_docs,\n",
    "                get_full_documents=get_full_documents,\n",
    "                load_all_available_meta=load_all_available_meta\n",
    "            )\n",
    "            \n",
    "            # Run the synchronous retriever in a thread pool\n",
    "            loop = asyncio.get_event_loop()\n",
    "            docs = await loop.run_in_executor(None, lambda: retriever.invoke(query))\n",
    "            \n",
    "            results = []\n",
    "            # Assign decreasing scores based on the order\n",
    "            base_score = 1.0\n",
    "            score_decrement = 1.0 / (len(docs) + 1) if docs else 0\n",
    "            \n",
    "            for i, doc in enumerate(docs):\n",
    "                # Extract metadata\n",
    "                metadata = doc.metadata\n",
    "                \n",
    "                # Use entry_id as the URL (this is the actual arxiv link)\n",
    "                url = metadata.get('entry_id', '')\n",
    "                \n",
    "                # Format content with all useful metadata\n",
    "                content_parts = []\n",
    "\n",
    "                # Primary information\n",
    "                if 'Summary' in metadata:\n",
    "                    content_parts.append(f\"Summary: {metadata['Summary']}\")\n",
    "\n",
    "                if 'Authors' in metadata:\n",
    "                    content_parts.append(f\"Authors: {metadata['Authors']}\")\n",
    "\n",
    "                # Add publication information\n",
    "                published = metadata.get('Published')\n",
    "                published_str = published.isoformat() if hasattr(published, 'isoformat') else str(published) if published else ''\n",
    "                if published_str:\n",
    "                    content_parts.append(f\"Published: {published_str}\")\n",
    "\n",
    "                # Add additional metadata if available\n",
    "                if 'primary_category' in metadata:\n",
    "                    content_parts.append(f\"Primary Category: {metadata['primary_category']}\")\n",
    "\n",
    "                if 'categories' in metadata and metadata['categories']:\n",
    "                    content_parts.append(f\"Categories: {', '.join(metadata['categories'])}\")\n",
    "\n",
    "                if 'comment' in metadata and metadata['comment']:\n",
    "                    content_parts.append(f\"Comment: {metadata['comment']}\")\n",
    "\n",
    "                if 'journal_ref' in metadata and metadata['journal_ref']:\n",
    "                    content_parts.append(f\"Journal Reference: {metadata['journal_ref']}\")\n",
    "\n",
    "                if 'doi' in metadata and metadata['doi']:\n",
    "                    content_parts.append(f\"DOI: {metadata['doi']}\")\n",
    "\n",
    "                # Get PDF link if available in the links\n",
    "                pdf_link = \"\"\n",
    "                if 'links' in metadata and metadata['links']:\n",
    "                    for link in metadata['links']:\n",
    "                        if 'pdf' in link:\n",
    "                            pdf_link = link\n",
    "                            content_parts.append(f\"PDF: {pdf_link}\")\n",
    "                            break\n",
    "\n",
    "                # Join all content parts with newlines \n",
    "                content = \"\\n\".join(content_parts)\n",
    "                \n",
    "                result = {\n",
    "                    'title': metadata.get('Title', ''),\n",
    "                    'url': url,  # Using entry_id as the URL\n",
    "                    'content': content,\n",
    "                    'score': base_score - (i * score_decrement),\n",
    "                    'raw_content': doc.page_content if get_full_documents else None\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            print(f\"Error processing arXiv query '{query}': {str(e)}\")\n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Process queries sequentially with delay to respect arXiv rate limit (1 request per 3 seconds)\n",
    "    search_docs = []\n",
    "    for i, query in enumerate(search_queries):\n",
    "        try:\n",
    "            # Add delay between requests (3 seconds per ArXiv's rate limit)\n",
    "            if i > 0:  # Don't delay the first request\n",
    "                await asyncio.sleep(3.0)\n",
    "            \n",
    "            result = await process_single_query(query)\n",
    "            search_docs.append(result)\n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            print(f\"Error processing arXiv query '{query}': {str(e)}\")\n",
    "            search_docs.append({\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            # Add additional delay if we hit a rate limit error\n",
    "            if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                print(\"ArXiv rate limit exceeded. Adding additional delay...\")\n",
    "                await asyncio.sleep(5.0)  # Add a longer delay if we hit a rate limit\n",
    "    \n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focused on retrieving biomedical literature, this function performs asynchronous searches on PubMed using a specialized API wrapper. For each query, it fetches a set of documents and processes them to extract important details such as the publication date, copyright information, and a summary of the research. It then constructs a URL for the PubMed entry and organizes everything neatly into a structured response. The function also dynamically adjusts delays between requests to avoid hitting rate limits, ensuring smooth and efficient retrieval of biomedical articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def pubmed_search_async(search_queries, top_k_results=5, email=None, api_key=None, doc_content_chars_max=4000):\n",
    "    \"\"\"\n",
    "    Performs concurrent searches on PubMed using the PubMedAPIWrapper.\n",
    "\n",
    "    Args:\n",
    "        search_queries (List[str]): List of search queries\n",
    "        top_k_results (int, optional): Maximum number of documents to return per query. Default is 5.\n",
    "        email (str, optional): Email address for PubMed API. Required by NCBI.\n",
    "        api_key (str, optional): API key for PubMed API for higher rate limits.\n",
    "        doc_content_chars_max (int, optional): Maximum characters for document content. Default is 4000.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of search responses from PubMed, one per query. Each response has format:\n",
    "            {\n",
    "                'query': str,                    # The original search query\n",
    "                'follow_up_questions': None,      \n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [                     # List of search results\n",
    "                    {\n",
    "                        'title': str,            # Title of the paper\n",
    "                        'url': str,              # URL to the paper on PubMed\n",
    "                        'content': str,          # Formatted summary with metadata\n",
    "                        'score': float,          # Relevance score (approximated)\n",
    "                        'raw_content': str       # Full abstract content\n",
    "                    },\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    async def process_single_query(query):\n",
    "        try:\n",
    "            # print(f\"Processing PubMed query: '{query}'\")\n",
    "            \n",
    "            # Create PubMed wrapper for the query\n",
    "            wrapper = PubMedAPIWrapper(\n",
    "                top_k_results=top_k_results,\n",
    "                doc_content_chars_max=doc_content_chars_max,\n",
    "                email=email if email else \"your_email@example.com\",\n",
    "                api_key=api_key if api_key else \"\"\n",
    "            )\n",
    "            \n",
    "            # Run the synchronous wrapper in a thread pool\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            # Use wrapper.lazy_load instead of load to get better visibility\n",
    "            docs = await loop.run_in_executor(None, lambda: list(wrapper.lazy_load(query)))\n",
    "            \n",
    "            print(f\"Query '{query}' returned {len(docs)} results\")\n",
    "            \n",
    "            results = []\n",
    "            # Assign decreasing scores based on the order\n",
    "            base_score = 1.0\n",
    "            score_decrement = 1.0 / (len(docs) + 1) if docs else 0\n",
    "            \n",
    "            for i, doc in enumerate(docs):\n",
    "                # Format content with metadata\n",
    "                content_parts = []\n",
    "                \n",
    "                if doc.get('Published'):\n",
    "                    content_parts.append(f\"Published: {doc['Published']}\")\n",
    "                \n",
    "                if doc.get('Copyright Information'):\n",
    "                    content_parts.append(f\"Copyright Information: {doc['Copyright Information']}\")\n",
    "                \n",
    "                if doc.get('Summary'):\n",
    "                    content_parts.append(f\"Summary: {doc['Summary']}\")\n",
    "                \n",
    "                # Generate PubMed URL from the article UID\n",
    "                uid = doc.get('uid', '')\n",
    "                url = f\"https://pubmed.ncbi.nlm.nih.gov/{uid}/\" if uid else \"\"\n",
    "                \n",
    "                # Join all content parts with newlines\n",
    "                content = \"\\n\".join(content_parts)\n",
    "                \n",
    "                result = {\n",
    "                    'title': doc.get('Title', ''),\n",
    "                    'url': url,\n",
    "                    'content': content,\n",
    "                    'score': base_score - (i * score_decrement),\n",
    "                    'raw_content': doc.get('Summary', '')\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Handle exceptions with more detailed information\n",
    "            error_msg = f\"Error processing PubMed query '{query}': {str(e)}\"\n",
    "            print(error_msg)\n",
    "            import traceback\n",
    "            print(traceback.format_exc())  # Print full traceback for debugging\n",
    "            \n",
    "            return {\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    # Process all queries with a reasonable delay between them\n",
    "    search_docs = []\n",
    "    \n",
    "    # Start with a small delay that increases if we encounter rate limiting\n",
    "    delay = 1.0  # Start with a more conservative delay\n",
    "    \n",
    "    for i, query in enumerate(search_queries):\n",
    "        try:\n",
    "            # Add delay between requests\n",
    "            if i > 0:  # Don't delay the first request\n",
    "                # print(f\"Waiting {delay} seconds before next query...\")\n",
    "                await asyncio.sleep(delay)\n",
    "            \n",
    "            result = await process_single_query(query)\n",
    "            search_docs.append(result)\n",
    "            \n",
    "            # If query was successful with results, we can slightly reduce delay (but not below minimum)\n",
    "            if result.get('results') and len(result['results']) > 0:\n",
    "                delay = max(0.5, delay * 0.9)  # Don't go below 0.5 seconds\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle exceptions gracefully\n",
    "            error_msg = f\"Error in main loop processing PubMed query '{query}': {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            search_docs.append({\n",
    "                'query': query,\n",
    "                'follow_up_questions': None,\n",
    "                'answer': None,\n",
    "                'images': [],\n",
    "                'results': [],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            \n",
    "            # If we hit an exception, increase delay for next query\n",
    "            delay = min(5.0, delay * 1.5)  # Don't exceed 5 seconds\n",
    "    \n",
    "    return search_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Planner and Configurations\n",
    "\n",
    "This code defines a configuration system for a chatbot that generates reports. Here's a breakdown of its components:\n",
    "\n",
    "1. **Default Report Structure:**  \n",
    "   A multi-line string provides a template for creating reports. It suggests starting with an introduction that gives an overview of the topic, then dividing the main content into sections focused on sub-topics, and finally ending with a conclusion that summarizes the main points.\n",
    "\n",
    "2. **Enumerated Types (Enums):**  \n",
    "   - **SearchAPI:** Lists possible search services (like PERPLEXITY, TAVILY, EXA, ARXIV, and PUBMED) that the chatbot might use to gather information.\n",
    "   - **PlannerProvider & WriterProvider:** These define options for external service providers that handle planning (organizing the report structure) and writing (generating the text). Options include providers like ANTHROPIC, OPENAI, and GROQ.\n",
    "\n",
    "3. **Configuration Data Class:**  \n",
    "   The `Configuration` class holds various settings for the chatbot. Key attributes include:\n",
    "   - **report_structure:** Uses the default report template.\n",
    "   - **number_of_queries:** Specifies how many search queries should be generated in each iteration (default is 2).\n",
    "   - **max_search_depth:** Sets the limit for how many times the chatbot can iterate through reflection and search (default is 2).\n",
    "   - **planner_provider & planner_model:** Determine which external service and model to use for planning the report.\n",
    "   - **writer_provider & writer_model:** Specify the external service and model for generating the report text.\n",
    "   - **search_api & search_api_config:** Indicate which search API to use (defaulting to TAVILY) and allow for additional API configuration.\n",
    "\n",
    "4. **Configuration Initialization Method:**  \n",
    "   The class method `from_runnable_config` allows the configuration to be created from a provided configuration object (or from environment variables). It checks for values in the environment (using uppercase names) and from a passed configuration dictionary. Only fields with provided values are used to instantiate the `Configuration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Any, Optional, Dict \n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from dataclasses import dataclass\n",
    "\n",
    "DEFAULT_REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "1. Introduction (no research needed)\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the report\n",
    "   \n",
    "Provide a paragraph with no more than 500 words to describe the key take aways on the topic\"\"\"\n",
    "\n",
    "class SearchAPI(Enum):\n",
    "    PERPLEXITY = \"perplexity\"\n",
    "    TAVILY = \"tavily\"\n",
    "    EXA = \"exa\"\n",
    "    ARXIV = \"arxiv\"\n",
    "    PUBMED = \"pubmed\"\n",
    "\n",
    "class PlannerProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    OPENAI = \"openai\"\n",
    "    GROQ = \"groq\"\n",
    "\n",
    "class WriterProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    OPENAI = \"openai\"\n",
    "    GROQ = \"groq\"\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "    report_structure: str = DEFAULT_REPORT_STRUCTURE # Defaults to the default report structure\n",
    "    number_of_queries: int = 2 # Number of search queries to generate per iteration\n",
    "    max_search_depth: int = 2 # Maximum number of reflection + search iterations\n",
    "    planner_provider: PlannerProvider = PlannerProvider.ANTHROPIC  # Defaults to Anthropic as provider\n",
    "    planner_model: str = \"claude-3-7-sonnet-latest\" # Defaults to claude-3-7-sonnet-latest, add \"-thinking\" to enable thinking mode\n",
    "    writer_provider: WriterProvider = WriterProvider.ANTHROPIC # Defaults to Anthropic as provider\n",
    "    writer_model: str = \"claude-3-5-sonnet-latest\" # Defaults to claude-3-5-sonnet-latest\n",
    "    search_api: SearchAPI = SearchAPI.TAVILY # Default to TAVILY\n",
    "    search_api_config: Optional[Dict[str, Any]] = None \n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These templates are used to guide the chatbot's behavior in different stages of report generation: \n",
    "\n",
    "1. **Report Planner Query Writer:**  \n",
    "   This prompt generates search queries to help with planning the report structure. It asks the model to create queries that will gather information for each section of the report.\n",
    "\n",
    "2. **Report Plan:**  \n",
    "   This prompt generates a plan for the report. It asks the model to create a list of sections for the report.\n",
    "\n",
    "3. **Section Writer:**  \n",
    "   This prompt generates a section of the report. It asks the model to write a section of the report based on the provided section topic and existing content.\n",
    "\n",
    "4. **Section Grader:**  \n",
    "   This prompt grades a section of the report. It asks the model to evaluate whether the section content adequately addresses the section topic.\n",
    "\n",
    "5. **Final Section Writer:**  \n",
    "   This prompt generates the final section of the report. It asks the model to write a section of the report that synthesizes information from the rest of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate search queries to help with planning the report\n",
    "report_planner_query_writer_instructions=\"\"\"You are performing research for a report. \n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Report organization>\n",
    "{report_organization}\n",
    "</Report organization>\n",
    "\n",
    "<Task>\n",
    "Your goal is to generate {number_of_queries} web search queries that will help gather information for planning the report sections. \n",
    "\n",
    "The queries should:\n",
    "\n",
    "1. Be related to the Report topic\n",
    "2. Help satisfy the requirements specified in the report organization\n",
    "\n",
    "Make the queries specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to generate the report plan\n",
    "report_planner_instructions=\"\"\"I want a plan for a report that is concise and focused.\n",
    "\n",
    "<Report topic>\n",
    "The topic of the report is:\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Report organization>\n",
    "The report should follow this organization: \n",
    "{report_organization}\n",
    "</Report organization>\n",
    "\n",
    "<Context>\n",
    "Here is context to use to plan the sections of the report: \n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "<Task>\n",
    "Generate a list of sections for the report. Your plan should be tight and focused with NO overlapping sections or unnecessary filler. \n",
    "\n",
    "For example, a good report structure might look like:\n",
    "1/ intro\n",
    "2/ overview of topic A\n",
    "3/ overview of topic B\n",
    "4/ comparison between A and B\n",
    "5/ conclusion\n",
    "\n",
    "Each section should have the fields:\n",
    "\n",
    "- Name - Name for this section of the report.\n",
    "- Description - Brief overview of the main topics covered in this section.\n",
    "- Research - Whether to perform web research for this section of the report.\n",
    "- Content - The content of the section, which you will leave blank for now.\n",
    "\n",
    "Integration guidelines:\n",
    "- Include examples and implementation details within main topic sections, not as separate sections\n",
    "- Ensure each section has a distinct purpose with no content overlap\n",
    "- Combine related concepts rather than separating them\n",
    "\n",
    "Before submitting, review your structure to ensure it has no redundant sections and follows a logical flow.\n",
    "</Task>\n",
    "\n",
    "<Feedback>\n",
    "Here is feedback on the report structure from review (if any):\n",
    "{feedback}\n",
    "</Feedback>\n",
    "\"\"\"\n",
    "\n",
    "# Query writer instructions\n",
    "query_writer_instructions=\"\"\"You are an expert technical writer crafting targeted web search queries that will gather comprehensive information for writing a technical report section.\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section topic>\n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Task>\n",
    "Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information above the section topic. \n",
    "\n",
    "The queries should:\n",
    "\n",
    "1. Be related to the topic \n",
    "2. Examine different aspects of the topic\n",
    "\n",
    "Make the queries specific enough to find high-quality, relevant sources.\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "# Section writer instructions\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer crafting one section of a technical report.\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section name>\n",
    "{section_name}\n",
    "</Section name>\n",
    "\n",
    "<Section topic>\n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Existing section content (if populated)>\n",
    "{section_content}\n",
    "</Existing section content>\n",
    "\n",
    "<Source material>\n",
    "{context}\n",
    "</Source material>\n",
    "\n",
    "<Guidelines for writing>\n",
    "1. If the existing section content is not populated, write a new section from scratch.\n",
    "2. If the existing section content is populated, write a new section that synthesizes the existing section content with the Source material.\n",
    "</Guidelines for writing>\n",
    "\n",
    "<Length and style>\n",
    "- Strict 150-200 word limit\n",
    "- No marketing language\n",
    "- Technical focus\n",
    "- Write in simple, clear language\n",
    "- Start with your most important insight in **bold**\n",
    "- Use short paragraphs (2-3 sentences max)\n",
    "- Use ## for section title (Markdown format)\n",
    "- Only use ONE structural element IF it helps clarify your point:\n",
    "  * Either a focused table comparing 2-3 key items (using Markdown table syntax)\n",
    "  * Or a short list (3-5 items) using proper Markdown list syntax:\n",
    "    - Use `*` or `-` for unordered lists\n",
    "    - Use `1.` for ordered lists\n",
    "    - Ensure proper indentation and spacing\n",
    "- End with ### Sources that references the below source material formatted as:\n",
    "  * List each source with title, date, and URL\n",
    "  * Format: `- Title : URL`\n",
    "</Length and style>\n",
    "\n",
    "<Quality checks>\n",
    "- Exactly 150-200 words (excluding title and sources)\n",
    "- Careful use of only ONE structural element (table or list) and only if it helps clarify your point\n",
    "- One specific example / case study\n",
    "- Starts with bold insight\n",
    "- No preamble prior to creating the section content\n",
    "- Sources cited at end\n",
    "</Quality checks>\n",
    "\"\"\"\n",
    "\n",
    "# Instructions for section grading\n",
    "section_grader_instructions = \"\"\"Review a report section relative to the specified topic:\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<section topic>\n",
    "{section_topic}\n",
    "</section topic>\n",
    "\n",
    "<section content>\n",
    "{section}\n",
    "</section content>\n",
    "\n",
    "<task>\n",
    "Evaluate whether the section content adequately addresses the section topic.\n",
    "\n",
    "If the section content does not adequately address the section topic, generate {number_of_follow_up_queries} follow-up search queries to gather missing information.\n",
    "</task>\n",
    "\n",
    "<format>\n",
    "    grade: Literal[\"pass\",\"fail\"] = Field(\n",
    "        description=\"Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail').\"\n",
    "    )\n",
    "    follow_up_queries: List[SearchQuery] = Field(\n",
    "        description=\"List of follow-up search queries.\",\n",
    "    )\n",
    "</format>\n",
    "\"\"\"\n",
    "\n",
    "final_section_writer_instructions=\"\"\"You are an expert technical writer crafting a section that synthesizes information from the rest of the report.\n",
    "\n",
    "<Report topic>\n",
    "{topic}\n",
    "</Report topic>\n",
    "\n",
    "<Section name>\n",
    "{section_name}\n",
    "</Section name>\n",
    "\n",
    "<Section topic> \n",
    "{section_topic}\n",
    "</Section topic>\n",
    "\n",
    "<Available report content>\n",
    "{context}\n",
    "</Available report content>\n",
    "\n",
    "<Task>\n",
    "1. Section-Specific Approach:\n",
    "\n",
    "For Introduction:\n",
    "- Use # for report title (Markdown format)\n",
    "- 50-100 word limit\n",
    "- Write in simple and clear language\n",
    "- Focus on the core motivation for the report in 1-2 paragraphs\n",
    "- Use a clear narrative arc to introduce the report\n",
    "- Include NO structural elements (no lists or tables)\n",
    "- No sources section needed\n",
    "\n",
    "For Conclusion/Summary:\n",
    "- Use ## for section title (Markdown format)\n",
    "- 100-150 word limit\n",
    "- For comparative reports:\n",
    "    * Must include a focused comparison table using Markdown table syntax\n",
    "    * Table should distill insights from the report\n",
    "    * Keep table entries clear and concise\n",
    "- For non-comparative reports: \n",
    "    * Only use ONE structural element IF it helps distill the points made in the report:\n",
    "    * Either a focused table comparing items present in the report (using Markdown table syntax)\n",
    "    * Or a short list using proper Markdown list syntax:\n",
    "      - Use `*` or `-` for unordered lists\n",
    "      - Use `1.` for ordered lists\n",
    "      - Ensure proper indentation and spacing\n",
    "- End with specific next steps or implications\n",
    "- No sources section needed\n",
    "\n",
    "3. Writing Approach:\n",
    "- Use concrete details over general statements\n",
    "- Make every word count\n",
    "- Focus on your single most important point\n",
    "</Task>\n",
    "\n",
    "<Quality Checks>\n",
    "- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section\n",
    "- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section\n",
    "- Markdown format\n",
    "- Do not include word count or any preamble in your response\n",
    "</Quality Checks>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes for Our Graph\n",
    "\n",
    "## 1. `generate_report_plan`\n",
    "\n",
    "**Purpose:**  \n",
    "Creates the initial report plan by breaking down the topic into sections and generating search queries to guide further research.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Input Extraction:** Retrieves the topic and any feedback provided.\n",
    "- **Configuration Loading:** Loads settings (e.g., report structure, number of queries, search API details).\n",
    "- **Query Generation:** Uses a writer model to generate search queries based on the topic and desired report organization.\n",
    "- **Web Search:** Executes a web search (via APIs like *tavily*, *perplexity*, *exa*, *arxiv*, or *pubmed*) with the generated queries to retrieve relevant sources.\n",
    "- **Section Planning:** Uses a planner model to create detailed sections (each with a name, description, plan, research flag, and content field) based on the gathered sources.\n",
    "- **Output:** Returns a dictionary with a key `\"sections\"` containing a list of planned sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# Nodes\n",
    "async def generate_report_plan(state: ReportState, config: RunnableConfig):\n",
    "    \"\"\" Generate the report plan \"\"\"\n",
    "\n",
    "    # Inputs\n",
    "    topic = state[\"topic\"]\n",
    "    feedback = state.get(\"feedback_on_report_plan\", None)\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    report_structure = configurable.report_structure\n",
    "    number_of_queries = configurable.number_of_queries\n",
    "    search_api = get_config_value(configurable.search_api)\n",
    "    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n",
    "    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n",
    "\n",
    "    # Convert JSON object to string if necessary\n",
    "    if isinstance(report_structure, dict):\n",
    "        report_structure = str(report_structure)\n",
    "\n",
    "    # Set writer model (model used for query writing and section writing)\n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0) \n",
    "    structured_llm = writer_model.with_structured_output(Queries)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)\n",
    "\n",
    "    # Generate queries  \n",
    "    results = structured_llm.invoke([SystemMessage(content=system_instructions_query),\n",
    "                                     HumanMessage(content=\"Generate search queries that will help with planning the sections of the report.\")])\n",
    "\n",
    "    # Web search\n",
    "    query_list = [query.search_query for query in results.queries]\n",
    "\n",
    "    # Search the web with parameters\n",
    "    if search_api == \"tavily\":\n",
    "        search_results = await tavily_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"perplexity\":\n",
    "        search_results = perplexity_search(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"exa\":\n",
    "        search_results = await exa_search(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"arxiv\":\n",
    "        search_results = await arxiv_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"pubmed\":\n",
    "        search_results = await pubmed_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported search API: {search_api}\")\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str, feedback=feedback)\n",
    "\n",
    "    # Set the planner\n",
    "    planner_provider = get_config_value(configurable.planner_provider)\n",
    "    planner_model = get_config_value(configurable.planner_model)\n",
    "\n",
    "    # Report planner instructions\n",
    "    planner_message = \"\"\"Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. \n",
    "                        Each section must have: name, description, plan, research, and content fields.\"\"\"\n",
    "\n",
    "    # Run the planner\n",
    "    if planner_model == \"claude-3-7-sonnet-latest-thinking\":\n",
    "\n",
    "        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n",
    "        planner_llm = init_chat_model(model=\"claude-3-7-sonnet-latest\", \n",
    "                                      model_provider=planner_provider, \n",
    "                                      max_tokens=20_000, \n",
    "                                      thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000})\n",
    "        \n",
    "        # with_structured_output uses forced tool calling, which thinking mode with Claude 3.7 does not support\n",
    "        # So, we use bind_tools without enforcing tool calling to generate the report sections\n",
    "        report_sections = planner_llm.bind_tools([Sections]).invoke([SystemMessage(content=system_instructions_sections),\n",
    "                                                                     HumanMessage(content=planner_message)])\n",
    "        tool_call = report_sections.tool_calls[0]['args']\n",
    "        report_sections = Sections.model_validate(tool_call)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # With other models, we can use with_structured_output\n",
    "        planner_llm = init_chat_model(model=planner_model, model_provider=planner_provider)\n",
    "        structured_llm = planner_llm.with_structured_output(Sections)\n",
    "        report_sections = structured_llm.invoke([SystemMessage(content=system_instructions_sections),\n",
    "                                                 HumanMessage(content=planner_message)])\n",
    "\n",
    "    # Get sections\n",
    "    sections = report_sections.sections\n",
    "\n",
    "    return {\"sections\": sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `human_feedback`\n",
    "\n",
    "**Purpose:**  \n",
    "Obtains user feedback on the generated report plan and decides whether to approve the plan or refine it.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Plan Formatting:** Converts the planned sections into a readable string format.\n",
    "- **Feedback Collection:** Uses an interrupt (user prompt) to gather approval or suggestions.\n",
    "- **Decision Making:**  \n",
    "  - If approved (feedback is `True`), triggers the building of report sections that require further research.  \n",
    "  - If not, updates the report plan with the provided feedback and regenerates the plan.\n",
    "- **Output:** Returns a command directing the next step—either to build sections or to re-run the planning node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback(state: ReportState, config: RunnableConfig) -> Command[Literal[\"generate_report_plan\",\"build_section_with_web_research\"]]:\n",
    "    \"\"\" Get feedback on the report plan \"\"\"\n",
    "\n",
    "    # Get sections\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state['sections']\n",
    "    sections_str = \"\\n\\n\".join(\n",
    "        f\"Section: {section.name}\\n\"\n",
    "        f\"Description: {section.description}\\n\"\n",
    "        f\"Research needed: {'Yes' if section.research else 'No'}\\n\"\n",
    "        for section in sections\n",
    "    )\n",
    "\n",
    "    # Get feedback on the report plan from interrupt\n",
    "    interrupt_message = f\"\"\"Please provide feedback on the following report plan. \n",
    "                        \\n\\n{sections_str}\\n\\n\n",
    "                        \\nDoes the report plan meet your needs? Pass 'true' to approve the report plan or provide feedback to regenerate the report plan:\"\"\"\n",
    "    \n",
    "    feedback = interrupt(interrupt_message)\n",
    "\n",
    "    # If the user approves the report plan, kick off section writing\n",
    "    if isinstance(feedback, bool) and feedback is True:\n",
    "        # Treat this as approve and kick off section writing\n",
    "        return Command(goto=[\n",
    "            Send(\"build_section_with_web_research\", {\"topic\": topic, \"section\": s, \"search_iterations\": 0}) \n",
    "            for s in sections \n",
    "            if s.research\n",
    "        ])\n",
    "    \n",
    "    # If the user provides feedback, regenerate the report plan \n",
    "    elif isinstance(feedback, str):\n",
    "        # Treat this as feedback\n",
    "        return Command(goto=\"generate_report_plan\", \n",
    "                       update={\"feedback_on_report_plan\": feedback})\n",
    "    else:\n",
    "        raise TypeError(f\"Interrupt value of type {type(feedback)} is not supported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `generate_queries`\n",
    "\n",
    "**Purpose:**  \n",
    "Generates targeted search queries for a specific report section to drive focused web research.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Input Extraction:** Uses the topic and section description.\n",
    "- **Query Generation:** Utilizes a writer model to generate a set number of search queries tailored to the section.\n",
    "- **Output:** Returns a dictionary with `\"search_queries\"` containing the list of generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\" Generate search queries for a report section \"\"\"\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    number_of_queries = configurable.number_of_queries\n",
    "\n",
    "    # Generate queries \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0) \n",
    "    structured_llm = writer_model.with_structured_output(Queries)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions = query_writer_instructions.format(topic=topic, \n",
    "                                                           section_topic=section.description, \n",
    "                                                           number_of_queries=number_of_queries)\n",
    "\n",
    "    # Generate queries  \n",
    "    queries = structured_llm.invoke([SystemMessage(content=system_instructions),\n",
    "                                     HumanMessage(content=\"Generate search queries on the provided topic.\")])\n",
    "\n",
    "    return {\"search_queries\": queries.queries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `search_web`\n",
    "\n",
    "**Purpose:**  \n",
    "Performs web searches using the generated queries to gather raw sources and relevant information for a report section.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Query List Preparation:** Converts the generated queries into a list.\n",
    "- **API Selection & Execution:** Calls the configured search API (e.g., *tavily*, *perplexity*, *exa*, etc.) with the appropriate parameters.\n",
    "- **Result Processing:** Deduplicates and formats the search results into a single string.\n",
    "- **Iteration Update:** Increments the search iteration count for tracking.\n",
    "- **Output:** Returns a dictionary with:\n",
    "  - `\"source_str\"`: The formatted search result string.\n",
    "  - `\"search_iterations\"`: The updated count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_web(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n",
    "    # Get state\n",
    "    search_queries = state[\"search_queries\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    search_api = get_config_value(configurable.search_api)\n",
    "    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty\n",
    "    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters\n",
    "\n",
    "    # Web search\n",
    "    query_list = [query.search_query for query in search_queries]\n",
    "\n",
    "    # Search the web with parameters\n",
    "    if search_api == \"tavily\":\n",
    "        search_results = await tavily_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=5000, include_raw_content=True)\n",
    "    elif search_api == \"perplexity\":\n",
    "        search_results = perplexity_search(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=5000, include_raw_content=False)\n",
    "    elif search_api == \"exa\":\n",
    "        search_results = await exa_search(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"arxiv\":\n",
    "        search_results = await arxiv_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    elif search_api == \"pubmed\":\n",
    "        search_results = await pubmed_search_async(query_list, **params_to_pass)\n",
    "        source_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, include_raw_content=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported search API: {search_api}\")\n",
    "\n",
    "    return {\"source_str\": source_str, \"search_iterations\": state[\"search_iterations\"] + 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `write_section`\n",
    "\n",
    "**Purpose:**  \n",
    "Writes the content of a report section by synthesizing the gathered web research.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Content Generation:** Uses a writer model to create section content based on the topic, section details, and the context provided by the search results.\n",
    "- **Content Validation:**  \n",
    "  - Employs a planner model to grade the generated section.  \n",
    "  - Determines if additional research is needed (by checking the grade or if the maximum search iterations are reached).\n",
    "- **Control Flow:**  \n",
    "  - If the section passes or maximum iterations are reached, it publishes the section to completed sections.  \n",
    "  - Otherwise, it updates the section with follow-up queries and loops back to `search_web`.\n",
    "- **Output:** Returns a command that either ends the search for this section or directs the next web search iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_section(state: SectionState, config: RunnableConfig) -> Command[Literal[END, \"search_web\"]]:\n",
    "    \"\"\" Write a section of the report \"\"\"\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "    source_str = state[\"source_str\"]\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # Format system instructions\n",
    "    system_instructions = section_writer_instructions.format(topic=topic, \n",
    "                                                             section_name=section.name, \n",
    "                                                             section_topic=section.description, \n",
    "                                                             context=source_str, \n",
    "                                                             section_content=section.content)\n",
    "\n",
    "    # Generate section  \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0) \n",
    "    section_content = writer_model.invoke([SystemMessage(content=system_instructions),\n",
    "                                           HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
    "    \n",
    "    # Write content to the section object  \n",
    "    section.content = section_content.content\n",
    "\n",
    "    # Grade prompt \n",
    "    section_grader_message = \"\"\"Grade the report and consider follow-up questions for missing information.\n",
    "                               If the grade is 'pass', return empty strings for all follow-up queries.\n",
    "                               If the grade is 'fail', provide specific search queries to gather missing information.\"\"\"\n",
    "    \n",
    "    section_grader_instructions_formatted = section_grader_instructions.format(topic=topic, \n",
    "                                                                               section_topic=section.description,\n",
    "                                                                               section=section.content, \n",
    "                                                                               number_of_follow_up_queries=configurable.number_of_queries)\n",
    "\n",
    "    # Use planner model for reflection\n",
    "    planner_provider = get_config_value(configurable.planner_provider)\n",
    "    planner_model = get_config_value(configurable.planner_model)\n",
    "\n",
    "    # If the planner model is claude-3-7-sonnet-latest, we need to use bind_tools to use thinking when generating the feedback \n",
    "    if planner_model == \"claude-3-7-sonnet-latest\":\n",
    "        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model\n",
    "        reflection_model = init_chat_model(model=planner_model, \n",
    "                                           model_provider=planner_provider, \n",
    "                                           max_tokens=20_000, \n",
    "                                           thinking={\"type\": \"enabled\", \"budget_tokens\": 16_000})\n",
    "        \n",
    "        # with_structured_output uses forced tool calling, which thinking mode with Claude 3.7 does not support\n",
    "        # So, we use bind_tools without enforcing tool calling to generate the report sections\n",
    "        reflection_result = reflection_model.bind_tools([Feedback]).invoke([SystemMessage(content=section_grader_instructions_formatted),\n",
    "                                                                            HumanMessage(content=section_grader_message)])\n",
    "        tool_call = reflection_result.tool_calls[0]['args']\n",
    "        feedback = Feedback.model_validate(tool_call)\n",
    "    \n",
    "    else:\n",
    "        reflection_model = init_chat_model(model=planner_model, \n",
    "                                           model_provider=planner_provider).with_structured_output(Feedback)\n",
    "        \n",
    "        feedback = reflection_model.invoke([SystemMessage(content=section_grader_instructions_formatted),\n",
    "                                            HumanMessage(content=section_grader_message)])\n",
    "\n",
    "    # If the section is passing or the max search depth is reached, publish the section to completed sections \n",
    "    if feedback.grade == \"pass\" or state[\"search_iterations\"] >= configurable.max_search_depth:\n",
    "        # Publish the section to completed sections \n",
    "        return  Command(\n",
    "        update={\"completed_sections\": [section]},\n",
    "        goto=END\n",
    "    )\n",
    "    # Update the existing section with new content and update search queries\n",
    "    else:\n",
    "        return  Command(\n",
    "        update={\"search_queries\": feedback.follow_up_queries, \"section\": section},\n",
    "        goto=\"search_web\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. `write_final_sections`\n",
    "\n",
    "**Purpose:**  \n",
    "Generates the final version of sections that do not require further research by using the compiled context from completed sections.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Context Preparation:** Receives the topic, section details, and the aggregated completed sections.\n",
    "- **Final Writing:** Uses a writer model to produce the final version of the section content.\n",
    "- **Output:** Returns a dictionary with `\"completed_sections\"` updated with the final section content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_final_sections(state: SectionState, config: RunnableConfig):\n",
    "    \"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n",
    "\n",
    "    # Get configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # Get state \n",
    "    topic = state[\"topic\"]\n",
    "    section = state[\"section\"]\n",
    "    completed_report_sections = state[\"report_sections_from_research\"]\n",
    "    \n",
    "    # Format system instructions\n",
    "    system_instructions = final_section_writer_instructions.format(topic=topic, section_name=section.name, section_topic=section.description, context=completed_report_sections)\n",
    "\n",
    "    # Generate section  \n",
    "    writer_provider = get_config_value(configurable.writer_provider)\n",
    "    writer_model_name = get_config_value(configurable.writer_model)\n",
    "    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, temperature=0) \n",
    "    section_content = writer_model.invoke([SystemMessage(content=system_instructions),\n",
    "                                           HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n",
    "    \n",
    "    # Write content to section \n",
    "    section.content = section_content.content\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. `gather_completed_sections`\n",
    "\n",
    "**Purpose:**  \n",
    "Consolidates all completed sections into a formatted context string to support final section writing.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Aggregation:** Collates completed sections from earlier research.\n",
    "- **Formatting:** Converts each section into a string format to be used as context.\n",
    "- **Output:** Returns a dictionary with `\"report_sections_from_research\"` containing the aggregated context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_completed_sections(state: ReportState):\n",
    "    \"\"\" Gather completed sections from research and format them as context for writing the final sections \"\"\"    \n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = format_sections(completed_sections)\n",
    "\n",
    "    return {\"report_sections_from_research\": completed_report_sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. `initiate_final_section_writing`\n",
    "\n",
    "**Purpose:**  \n",
    "Triggers the final writing phase for report sections that do not need further web research.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Selection:** Identifies sections marked as not requiring additional research.\n",
    "- **Parallel Processing:** Uses a parallelized `Send()` API to launch final section writing tasks concurrently.\n",
    "- **Output:** Returns a list of `Send` commands for writing final sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_final_section_writing(state: ReportState):\n",
    "    \"\"\" Write any final sections using the Send API to parallelize the process \"\"\"    \n",
    "\n",
    "    # Kick off section writing in parallel via Send() API for any sections that do not require research\n",
    "    return [\n",
    "        Send(\"write_final_sections\", {\"topic\": state[\"topic\"], \"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n",
    "        for s in state[\"sections\"] \n",
    "        if not s.research\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. `compile_final_report`\n",
    "\n",
    "**Purpose:**  \n",
    "Compiles all finalized sections into one cohesive final report.\n",
    "\n",
    "**Key Steps:**\n",
    "- **Content Mapping:** Matches the finalized content with the original sections while preserving the intended order.\n",
    "- **Report Assembly:** Joins all section contents together into a single text string.\n",
    "- **Output:** Returns a dictionary with the key `\"final_report\"` containing the complete report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_final_report(state: ReportState):\n",
    "    \"\"\" Compile the final report \"\"\"    \n",
    "\n",
    "    # Get sections\n",
    "    sections = state[\"sections\"]\n",
    "    completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n",
    "\n",
    "    # Update sections with completed content while maintaining original order\n",
    "    for section in sections:\n",
    "        section.content = completed_sections[section.name]\n",
    "\n",
    "    # Compile final report\n",
    "    all_sections = \"\\n\\n\".join([s.content for s in sections])\n",
    "\n",
    "    return {\"final_report\": all_sections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD THAT GRAPH!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1203de9f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_builder = StateGraph(SectionState, output=SectionOutputState)\n",
    "section_builder.add_node(\"generate_queries\", generate_queries)\n",
    "section_builder.add_node(\"search_web\", search_web)\n",
    "section_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# Add edges\n",
    "section_builder.add_edge(START, \"generate_queries\")\n",
    "section_builder.add_edge(\"generate_queries\", \"search_web\")\n",
    "section_builder.add_edge(\"search_web\", \"write_section\")\n",
    "\n",
    "# Outer graph -- \n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=Configuration)\n",
    "builder.add_node(\"generate_report_plan\", generate_report_plan)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"build_section_with_web_research\", section_builder.compile())\n",
    "builder.add_node(\"gather_completed_sections\", gather_completed_sections)\n",
    "builder.add_node(\"write_final_sections\", write_final_sections)\n",
    "builder.add_node(\"compile_final_report\", compile_final_report)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_report_plan\")\n",
    "builder.add_edge(\"generate_report_plan\", \"human_feedback\")\n",
    "builder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\n",
    "builder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\n",
    "builder.add_edge(\"write_final_sections\", \"compile_final_report\")\n",
    "builder.add_edge(\"compile_final_report\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Graph: WITH CHECKPOINTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Create a memory saver for checkpointing\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer\n",
    "graph_with_checkpoint = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(graph_with_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(graph_with_checkpoint.get_graph(xray=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mermaid graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph_with_checkpoint.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_report_plan': {'sections': [Section(name='Introduction to DeepSeek-R1', description='Brief overview of DeepSeek-R1, its development by the Chinese AI startup DeepSeek, and its positioning in the AI landscape as a reasoning-focused large language model.', research=False, content=''), Section(name='Technical Architecture and Capabilities', description=\"Detailed explanation of DeepSeek-R1's architecture, its reasoning abilities, mathematical problem-solving capabilities, and code generation strengths.\", research=True, content=''), Section(name='DeepSeek-R1 Model Variants', description='Overview of the different DeepSeek-R1 models, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled models based on Llama and Qwen architectures.', research=True, content=''), Section(name='Performance Benchmarks and Comparisons', description=\"Analysis of DeepSeek-R1's performance across various benchmarks with direct comparisons to competing models like OpenAI's o1, focusing on reasoning, coding, and mathematical capabilities.\", research=True, content=''), Section(name='Accessibility and Pricing', description='Information about how to access DeepSeek-R1 through APIs, platforms like Azure AI Foundry, implementation options, and its cost-efficiency compared to other leading AI models.', research=True, content=''), Section(name='Conclusion: Impact and Future Prospects', description=\"Summary of DeepSeek-R1's significance in the AI ecosystem, its key advantages and limitations, and its potential impact on future AI development and applications.\", research=False, content='')]}}\n",
      "\n",
      "\n",
      "{'__interrupt__': (Interrupt(value=\"Please provide feedback on the following report plan. \\n                        \\n\\nSection: Introduction to DeepSeek-R1\\nDescription: Brief overview of DeepSeek-R1, its development by the Chinese AI startup DeepSeek, and its positioning in the AI landscape as a reasoning-focused large language model.\\nResearch needed: No\\n\\n\\nSection: Technical Architecture and Capabilities\\nDescription: Detailed explanation of DeepSeek-R1's architecture, its reasoning abilities, mathematical problem-solving capabilities, and code generation strengths.\\nResearch needed: Yes\\n\\n\\nSection: DeepSeek-R1 Model Variants\\nDescription: Overview of the different DeepSeek-R1 models, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled models based on Llama and Qwen architectures.\\nResearch needed: Yes\\n\\n\\nSection: Performance Benchmarks and Comparisons\\nDescription: Analysis of DeepSeek-R1's performance across various benchmarks with direct comparisons to competing models like OpenAI's o1, focusing on reasoning, coding, and mathematical capabilities.\\nResearch needed: Yes\\n\\n\\nSection: Accessibility and Pricing\\nDescription: Information about how to access DeepSeek-R1 through APIs, platforms like Azure AI Foundry, implementation options, and its cost-efficiency compared to other leading AI models.\\nResearch needed: Yes\\n\\n\\nSection: Conclusion: Impact and Future Prospects\\nDescription: Summary of DeepSeek-R1's significance in the AI ecosystem, its key advantages and limitations, and its potential impact on future AI development and applications.\\nResearch needed: No\\n\\n\\n\\n                        \\nDoes the report plan meet your needs? Pass 'true' to approve the report plan or provide feedback to regenerate the report plan:\", resumable=True, ns=['human_feedback:5cfed1cf-deb0-dc36-cf2e-bd761d5bea6f'], when='during'),)}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Feedback Request:**\n",
       "Please provide feedback on the following report plan. \n",
       "                        \n",
       "\n",
       "Section: Introduction to DeepSeek-R1\n",
       "Description: Brief overview of DeepSeek-R1, its development by the Chinese AI startup DeepSeek, and its positioning in the AI landscape as a reasoning-focused large language model.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "Section: Technical Architecture and Capabilities\n",
       "Description: Detailed explanation of DeepSeek-R1's architecture, its reasoning abilities, mathematical problem-solving capabilities, and code generation strengths.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: DeepSeek-R1 Model Variants\n",
       "Description: Overview of the different DeepSeek-R1 models, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled models based on Llama and Qwen architectures.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Performance Benchmarks and Comparisons\n",
       "Description: Analysis of DeepSeek-R1's performance across various benchmarks with direct comparisons to competing models like OpenAI's o1, focusing on reasoning, coding, and mathematical capabilities.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Accessibility and Pricing\n",
       "Description: Information about how to access DeepSeek-R1 through APIs, platforms like Azure AI Foundry, implementation options, and its cost-efficiency compared to other leading AI models.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Conclusion: Impact and Future Prospects\n",
       "Description: Summary of DeepSeek-R1's significance in the AI ecosystem, its key advantages and limitations, and its potential impact on future AI development and applications.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "\n",
       "                        \n",
       "Does the report plan meet your needs? Pass 'true' to approve the report plan or provide feedback to regenerate the report plan:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a unique thread ID\n",
    "import uuid\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Start the graph execution with the topic and display the final report when it appears\n",
    "async def run_graph_and_show_report():\n",
    "    \"\"\"Run the graph and display the final report when it appears\"\"\"\n",
    "    async for chunk in graph_with_checkpoint.astream(\n",
    "        {\"topic\": \"DeepSeek-R1\"}, \n",
    "        {\"configurable\": {\"thread_id\": thread_id}},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(chunk)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Check if this chunk contains the final_report\n",
    "        if isinstance(chunk, dict) and 'final_report' in chunk:\n",
    "            print(\"🎉 Final report generated! 🎉\")\n",
    "            display(Markdown(f\"# DeepSeek-R1 Report\\n\\n{chunk['final_report']}\"))\n",
    "            return\n",
    "        \n",
    "        # Check if this is an interrupt that needs user feedback\n",
    "        if isinstance(chunk, dict) and '__interrupt__' in chunk:\n",
    "            interrupt_value = chunk['__interrupt__'][0].value\n",
    "            display(Markdown(f\"**Feedback Request:**\\n{interrupt_value}\"))\n",
    "            return  # Stop execution to allow user to provide feedback\n",
    "\n",
    "# Run the graph\n",
    "await run_graph_and_show_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def approve_plan():\n",
    "    \"\"\"Approve the plan and continue execution\"\"\"\n",
    "    async for chunk in graph_with_checkpoint.astream(\n",
    "        Command(resume=True), \n",
    "        {\"configurable\": {\"thread_id\": thread_id}},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(chunk)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Check if this chunk contains the compile_final_report with final_report\n",
    "        if isinstance(chunk, dict) and 'compile_final_report' in chunk:\n",
    "            if 'final_report' in chunk['compile_final_report']:\n",
    "                print(\"🎉 Final report generated! 🎉\")\n",
    "                final_report = chunk['compile_final_report']['final_report']\n",
    "                display(Markdown(f\"# DeepSeek-R1 Report\\n\\n{final_report}\"))\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def provide_feedback(feedback_text):\n",
    "    \"\"\"Provide feedback and continue execution\"\"\"\n",
    "    async for chunk in graph_with_checkpoint.astream(\n",
    "        Command(resume=feedback_text), \n",
    "        {\"configurable\": {\"thread_id\": thread_id}},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(chunk)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Check if this chunk contains the final_report\n",
    "        if isinstance(chunk, dict) and 'final_report' in chunk:\n",
    "            print(\"🎉 Final report generated! 🎉\")\n",
    "            display(Markdown(f\"# DeepSeek-R1 Report\\n\\n{chunk['final_report']}\"))\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: You *can* choose to continue the flow - though the notebook implementation will require you to stretch your coding muscles a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human_feedback': {'feedback_on_report_plan': 'This report is missing the section on the history of the company.'}}\n",
      "\n",
      "\n",
      "{'generate_report_plan': {'sections': [Section(name='Introduction to DeepSeek-R1', description='Brief overview of DeepSeek-R1, its significance in the AI landscape, and the main capabilities that distinguish it as a reasoning-focused language model.', research=False, content=''), Section(name='Technical Architecture and Capabilities', description='Deep dive into the model architecture, parameters, and technical specifications of DeepSeek-R1, including its reasoning abilities and performance benchmarks.', research=True, content=''), Section(name='DeepSeek-R1 Family and Distilled Models', description='Overview of the DeepSeek-R1 model variants including DeepSeek-R1-Zero and the distilled smaller models based on Llama and Qwen architectures.', research=True, content=''), Section(name='Comparative Analysis: DeepSeek-R1 vs. Competitors', description=\"Comparison of DeepSeek-R1 with other leading models like OpenAI's o1 series, examining strengths, limitations, and unique advantages.\", research=True, content=''), Section(name='Accessibility and Implementation', description='Information on how to access and implement DeepSeek-R1, including API integration, deployment options, and pricing structure.', research=True, content=''), Section(name='DeepSeek Company Background', description='Brief history of DeepSeek as a company, its founding, development trajectory, and position in the AI industry landscape.', research=True, content=''), Section(name='Conclusion: Impact and Future Prospects', description=\"Summary of DeepSeek-R1's significance, current impact on the AI field, and potential future developments, with a structured comparison table of key features.\", research=False, content='')]}}\n",
      "\n",
      "\n",
      "{'__interrupt__': (Interrupt(value=\"Please provide feedback on the following report plan. \\n                        \\n\\nSection: Introduction to DeepSeek-R1\\nDescription: Brief overview of DeepSeek-R1, its significance in the AI landscape, and the main capabilities that distinguish it as a reasoning-focused language model.\\nResearch needed: No\\n\\n\\nSection: Technical Architecture and Capabilities\\nDescription: Deep dive into the model architecture, parameters, and technical specifications of DeepSeek-R1, including its reasoning abilities and performance benchmarks.\\nResearch needed: Yes\\n\\n\\nSection: DeepSeek-R1 Family and Distilled Models\\nDescription: Overview of the DeepSeek-R1 model variants including DeepSeek-R1-Zero and the distilled smaller models based on Llama and Qwen architectures.\\nResearch needed: Yes\\n\\n\\nSection: Comparative Analysis: DeepSeek-R1 vs. Competitors\\nDescription: Comparison of DeepSeek-R1 with other leading models like OpenAI's o1 series, examining strengths, limitations, and unique advantages.\\nResearch needed: Yes\\n\\n\\nSection: Accessibility and Implementation\\nDescription: Information on how to access and implement DeepSeek-R1, including API integration, deployment options, and pricing structure.\\nResearch needed: Yes\\n\\n\\nSection: DeepSeek Company Background\\nDescription: Brief history of DeepSeek as a company, its founding, development trajectory, and position in the AI industry landscape.\\nResearch needed: Yes\\n\\n\\nSection: Conclusion: Impact and Future Prospects\\nDescription: Summary of DeepSeek-R1's significance, current impact on the AI field, and potential future developments, with a structured comparison table of key features.\\nResearch needed: No\\n\\n\\n\\n                        \\nDoes the report plan meet your needs? Pass 'true' to approve the report plan or provide feedback to regenerate the report plan:\", resumable=True, ns=['human_feedback:9a85ccc3-8128-ed7e-476b-42fbc7a878c4'], when='during'),)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await provide_feedback(\"This report is missing the section on the history of the company.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'build_section_with_web_research': {'completed_sections': [Section(name='DeepSeek-R1 Family and Distilled Models', description='Overview of the DeepSeek-R1 model variants including DeepSeek-R1-Zero and the distilled smaller models based on Llama and Qwen architectures.', research=True, content=\"## DeepSeek-R1 Family and Distilled Models\\n\\n**DeepSeek has created a comprehensive family of reasoning-focused models, with the flagship R1 model being distilled into smaller, more practical versions while maintaining strong reasoning capabilities.** The full DeepSeek-R1 model contains 671B parameters but only activates 37B during inference through its Mixture of Experts architecture.\\n\\nTo make this technology more accessible, DeepSeek developed six distilled models based on Qwen and Llama architectures, ranging from 1.5B to 70B parameters. These models are trained using 800,000 curated samples generated by the full R1 model.\\n\\nThe distilled models show impressive performance relative to their size. For example, the Qwen-32B distilled version achieves 72.6% accuracy on AIME 2024 mathematical reasoning tasks, approaching the full R1's 79.8% while being significantly more deployable.\\n\\nThe models follow a clear performance hierarchy:\\n- Large distills (32B-70B): Near full R1 performance\\n- Medium distills (7B-14B): Strong reasoning, practical deployment\\n- Small distills (1.5B-8B): Basic reasoning capabilities, highly efficient\\n\\n### Sources\\n- Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero : https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero\\n- Reality Check: Distilled Models ≠ Full Model : https://www.groff.dev/blog/reality-check-distilled-models\\n- What are DeepSeek-R1 distilled models? : https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\")]}, '__metadata__': {'cached': True}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Accessibility and Implementation', description='Information on how to access and implement DeepSeek-R1, including API integration, deployment options, and pricing structure.', research=True, content='## Accessibility and Implementation\\n\\n**DeepSeek-R1 offers multiple deployment options with varying hardware requirements, making it accessible through three primary methods: web interface, local installation, or API integration.** The web interface at chat.deepseek.com provides the simplest access point, while local deployment requires more technical setup but offers greater control and privacy.\\n\\nFor local installation, the model is available in different sizes to accommodate various hardware configurations. The lightweight 1.5B parameter version requires only 3.5GB RAM and an RTX 3060, while the full 671B parameter model demands substantial resources including 1,342GB RAM and multiple A100 GPUs.\\n\\nImplementation tools like Ollama and Docker simplify deployment. Ollama offers straightforward installation through command-line interface, while Docker provides containerized deployment with persistent storage. The API follows OpenAI-compatible formatting and includes intelligent caching that can reduce costs by up to 90% for repeated queries.\\n\\nPricing for API usage is structured per million tokens:\\n* Input (cache miss): $0.55\\n* Input (cache hit): $0.14\\n* Output: $2.19\\n\\n### Sources\\n- How to Run DeepSeek R1 Locally: Step-by-Step Process : https://techwiser.com/how-to-run-deepseek-r1-locally-step-by-step-process/\\n- DeepSeek R1 Review: API Pricing & How to Use DeepSeek R1 API : https://apidog.com/blog/deepseek-r1-review-api/\\n- Run DeepSeek R1 Distilled Locally (Web Interface) : https://habr.com/en/articles/881140/')]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='DeepSeek Company Background', description='Brief history of DeepSeek as a company, its founding, development trajectory, and position in the AI industry landscape.', research=True, content='## DeepSeek Company Background\\n\\n**DeepSeek represents a new model of AI company development, emerging from High-Flyer\\'s quantitative trading division to become a standalone AI powerhouse that achieved breakthrough capabilities with remarkably efficient resource utilization**. Founded in May 2023 by Liang Wenfeng, the company has demonstrated that massive computing infrastructure and multibillion-dollar budgets aren\\'t prerequisites for AI innovation.\\n\\nDeepSeek\\'s development trajectory includes several key milestones:\\n\\n- 2019: Initial AI research division established within High-Flyer\\n- 2023: Spin-off as independent entity\\n- 2024: Release of DeepSeek-V2 and V3 models\\n- 2025: Consideration of first external funding round\\n\\nThe company gained prominence for developing high-performance AI models at a fraction of competitors\\' costs. Most notably, their DeepSeek-V3 model, developed for approximately $5.58 million, achieved performance comparable to industry leaders while using significantly fewer resources.\\n\\nCurrently, DeepSeek maintains a research-focused approach with limited commercialization. Primarily funded by High-Flyer, the company has earned recognition as the \"Pinduoduo of AI\" for its cost-efficient development strategy that challenges conventional assumptions about necessary AI investment scales.\\n\\n### Sources\\n- DeepSeek has accelerated the race for global AI dominance: https://www.businessthink.unsw.edu.au/articles/deepseek-china-ai-industry-development\\n- DeepSeek sparks debate over Big Tech\\'s investment strategies: https://www.usatoday.com/story/money/2025/01/29/deepseek-big-tech-overspending-investments/78028924007/\\n- Industry Leaders React to DeepSeek\\'s Impact on the Market: https://futureweek.com/industry-leaders-react-to-deepseek/')]}}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (0cfdb608-8241-4f8c-b80c-0e8652a7b822) of 80,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# lgtm\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m approve_plan()\n",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m, in \u001b[0;36mapprove_plan\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapprove_plan\u001b[39m():\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Approve the plan and continue execution\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m graph_with_checkpoint\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m      4\u001b[0m         Command(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: thread_id}},\n\u001b[1;32m      6\u001b[0m         stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     ):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(chunk)\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2274\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2274\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2275\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2276\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2277\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2278\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2279\u001b[0m     ):\n\u001b[1;32m   2280\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2282\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:527\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    525\u001b[0m     fut\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_exc_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeoutError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:619\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m panic:\n\u001b[0;32m--> 619\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:583\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    580\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2389\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2388\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2389\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   2390\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2391\u001b[0m     config,\n\u001b[1;32m   2392\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   2393\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2394\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   2395\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   2396\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   2397\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2398\u001b[0m ):\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2400\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2274\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2274\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2275\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2276\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2277\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2278\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2279\u001b[0m     ):\n\u001b[1;32m   2280\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2282\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:444\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    442\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    445\u001b[0m         t,\n\u001b[1;32m    446\u001b[0m         retry_policy,\n\u001b[1;32m    447\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    448\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    449\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    450\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    451\u001b[0m         },\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:583\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    580\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    370\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 371\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:588\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    590\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.0-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:579\u001b[0m, in \u001b[0;36mrun_in_executor.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36mwrite_section\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     21\u001b[0m writer_model_name \u001b[38;5;241m=\u001b[39m get_config_value(configurable\u001b[38;5;241m.\u001b[39mwriter_model)\n\u001b[1;32m     22\u001b[0m writer_model \u001b[38;5;241m=\u001b[39m init_chat_model(model\u001b[38;5;241m=\u001b[39mwriter_model_name, model_provider\u001b[38;5;241m=\u001b[39mwriter_provider, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m---> 23\u001b[0m section_content \u001b[38;5;241m=\u001b[39m \u001b[43mwriter_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_instructions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGenerate a report section based on the provided sources.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Write content to the section object  \u001b[39;00m\n\u001b[1;32m     27\u001b[0m section\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m=\u001b[39m section_content\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/langchain_anthropic/chat_models.py:849\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    848\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 849\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/resources/messages/messages.py:953\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    947\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    950\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    951\u001b[0m     )\n\u001b[0;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthinking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1336\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1324\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1333\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1334\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1335\u001b[0m     )\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1102\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1101\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1151\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1102\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1101\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1151\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/AIE5/14_Open_DeepResearch/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1117\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1116\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1120\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1121\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1126\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (0cfdb608-8241-4f8c-b80c-0e8652a7b822) of 80,000 input tokens per minute. For details, refer to: https://docs.anthropic.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
      "\u001b[0mDuring task with name 'write_section' and id 'c5d174c6-5c4c-d2a7-6fdd-89354311153b'",
      "\u001b[0mDuring task with name 'build_section_with_web_research' and id 'b83d9549-52e7-56d2-9eee-f0e4e6306ce5'"
     ]
    }
   ],
   "source": [
    "# lgtm\n",
    "await approve_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'build_section_with_web_research': {'completed_sections': [Section(name='DeepSeek-R1 Family and Distilled Models', description='Overview of the DeepSeek-R1 model variants including DeepSeek-R1-Zero and the distilled smaller models based on Llama and Qwen architectures.', research=True, content=\"## DeepSeek-R1 Family and Distilled Models\\n\\n**DeepSeek has created a comprehensive family of reasoning-focused models, with the flagship R1 model being distilled into smaller, more practical versions while maintaining strong reasoning capabilities.** The full DeepSeek-R1 model contains 671B parameters but only activates 37B during inference through its Mixture of Experts architecture.\\n\\nTo make this technology more accessible, DeepSeek developed six distilled models based on Qwen and Llama architectures, ranging from 1.5B to 70B parameters. These models are trained using 800,000 curated samples generated by the full R1 model.\\n\\nThe distilled models show impressive performance relative to their size. For example, the Qwen-32B distilled version achieves 72.6% accuracy on AIME 2024 mathematical reasoning tasks, approaching the full R1's 79.8% while being significantly more deployable.\\n\\nThe models follow a clear performance hierarchy:\\n- Large distills (32B-70B): Near full R1 performance\\n- Medium distills (7B-14B): Strong reasoning, practical deployment\\n- Small distills (1.5B-8B): Basic reasoning capabilities, highly efficient\\n\\n### Sources\\n- Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero : https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero\\n- Reality Check: Distilled Models ≠ Full Model : https://www.groff.dev/blog/reality-check-distilled-models\\n- What are DeepSeek-R1 distilled models? : https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\")]}, '__metadata__': {'cached': True}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Accessibility and Implementation', description='Information on how to access and implement DeepSeek-R1, including API integration, deployment options, and pricing structure.', research=True, content='## Accessibility and Implementation\\n\\n**DeepSeek-R1 offers multiple deployment options with varying hardware requirements, making it accessible through three primary methods: web interface, local installation, or API integration.** The web interface at chat.deepseek.com provides the simplest access point, while local deployment requires more technical setup but offers greater control and privacy.\\n\\nFor local installation, the model is available in different sizes to accommodate various hardware configurations. The lightweight 1.5B parameter version requires only 3.5GB RAM and an RTX 3060, while the full 671B parameter model demands substantial resources including 1,342GB RAM and multiple A100 GPUs.\\n\\nImplementation tools like Ollama and Docker simplify deployment. Ollama offers straightforward installation through command-line interface, while Docker provides containerized deployment with persistent storage. The API follows OpenAI-compatible formatting and includes intelligent caching that can reduce costs by up to 90% for repeated queries.\\n\\nPricing for API usage is structured per million tokens:\\n* Input (cache miss): $0.55\\n* Input (cache hit): $0.14\\n* Output: $2.19\\n\\n### Sources\\n- How to Run DeepSeek R1 Locally: Step-by-Step Process : https://techwiser.com/how-to-run-deepseek-r1-locally-step-by-step-process/\\n- DeepSeek R1 Review: API Pricing & How to Use DeepSeek R1 API : https://apidog.com/blog/deepseek-r1-review-api/\\n- Run DeepSeek R1 Distilled Locally (Web Interface) : https://habr.com/en/articles/881140/')]}, '__metadata__': {'cached': True}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='DeepSeek Company Background', description='Brief history of DeepSeek as a company, its founding, development trajectory, and position in the AI industry landscape.', research=True, content='## DeepSeek Company Background\\n\\n**DeepSeek represents a new model of AI company development, emerging from High-Flyer\\'s quantitative trading division to become a standalone AI powerhouse that achieved breakthrough capabilities with remarkably efficient resource utilization**. Founded in May 2023 by Liang Wenfeng, the company has demonstrated that massive computing infrastructure and multibillion-dollar budgets aren\\'t prerequisites for AI innovation.\\n\\nDeepSeek\\'s development trajectory includes several key milestones:\\n\\n- 2019: Initial AI research division established within High-Flyer\\n- 2023: Spin-off as independent entity\\n- 2024: Release of DeepSeek-V2 and V3 models\\n- 2025: Consideration of first external funding round\\n\\nThe company gained prominence for developing high-performance AI models at a fraction of competitors\\' costs. Most notably, their DeepSeek-V3 model, developed for approximately $5.58 million, achieved performance comparable to industry leaders while using significantly fewer resources.\\n\\nCurrently, DeepSeek maintains a research-focused approach with limited commercialization. Primarily funded by High-Flyer, the company has earned recognition as the \"Pinduoduo of AI\" for its cost-efficient development strategy that challenges conventional assumptions about necessary AI investment scales.\\n\\n### Sources\\n- DeepSeek has accelerated the race for global AI dominance: https://www.businessthink.unsw.edu.au/articles/deepseek-china-ai-industry-development\\n- DeepSeek sparks debate over Big Tech\\'s investment strategies: https://www.usatoday.com/story/money/2025/01/29/deepseek-big-tech-overspending-investments/78028924007/\\n- Industry Leaders React to DeepSeek\\'s Impact on the Market: https://futureweek.com/industry-leaders-react-to-deepseek/')]}, '__metadata__': {'cached': True}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Comparative Analysis: DeepSeek-R1 vs. Competitors', description=\"Comparison of DeepSeek-R1 with other leading models like OpenAI's o1 series, examining strengths, limitations, and unique advantages.\", research=True, content=\"## Comparative Analysis: DeepSeek-R1 vs. Competitors\\n\\n**DeepSeek-R1 demonstrates superior performance in mathematical and logical reasoning tasks while offering significant cost advantages through its open-source architecture, though it faces limitations in creative and general knowledge tasks.**\\n\\nKey performance metrics reveal DeepSeek-R1's strengths and weaknesses compared to leading competitors:\\n\\nModel | Reasoning | General Knowledge | Cost per 1M Tokens\\n---|---|---|---\\nDeepSeek-R1 | 90.2% | 71.5% | $0.50\\nOpenAI o1 | 85.3% | 77.3% | $2.50\\nGoogle Gemini | 88.0% | 75.7% | $0.10\\n\\nIn real-world applications, DeepSeek-R1 excels in structured problem-solving. For example, in financial risk analysis, it achieved 92% accuracy in complex algebraic calculations compared to OpenAI o1's 85%, demonstrating its strength in mathematical reasoning.\\n\\nHowever, DeepSeek-R1 shows limitations in creative tasks and general knowledge queries. Its responses to open-ended questions about current events often lack the contextual richness found in competitors' outputs. The model's focus on logical reasoning comes at the cost of reduced performance in more general applications.\\n\\n### Sources\\n- DeepSeek R1 vs OpenAI O1: AI Model Comparison (2025): https://www.zignuts.com/blog/deepseek-r1-vs-openai-o1-comparison\\n- DeepSeek R1: Complete analysis of capabilities and limitations: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\\n- DeepSeek R1 Explained: Features, Benefits, and Use Cases: https://fastbots.ai/blog/deepseek-r1-explained-features-benefits-and-use-cases\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Technical Architecture and Capabilities', description='Deep dive into the model architecture, parameters, and technical specifications of DeepSeek-R1, including its reasoning abilities and performance benchmarks.', research=True, content=\"## Technical Architecture and Capabilities\\n\\n**DeepSeek-R1's breakthrough architecture activates only 37B of its 671B parameters per token through an innovative Mixture-of-Experts (MoE) design, dramatically improving computational efficiency while maintaining performance.**\\n\\nThe model's core architecture combines three key innovations. Multi-Head Latent Attention (MLA) compresses Key-Value vectors into a latent space, reducing memory footprint by 57x compared to traditional attention mechanisms. DeepSeekMoE incorporates dynamic bias terms for load balancing without auxiliary losses, while shared experts enhance generalization across tasks.\\n\\nThe architecture includes advanced features for optimization:\\n\\n* FP8 mixed precision training that halves memory usage\\n* DualPipe parallelism for efficient communication\\n* Multi-token prediction enabling faster inference\\n* 128K context window with adaptive caching\\n\\nIn benchmarks, this architecture delivers exceptional performance, achieving 97.3% accuracy on MATH-500 and a 2,029 Elo rating on Codeforces. The model demonstrates particular strength in reasoning tasks through its novel training approach combining reinforcement learning with supervised fine-tuning stages.\\n\\n### Sources\\n- Model Architecture Behind DeepSeek R1 - Founders Creative : https://www.founderscreative.org/model-architecture-behind-deepseek-r1/\\n- DeepSeek-V3 (and R1!) Architecture | by Gal Hyams : https://medium.com/@galhyams/deepseek-v3-and-r1-architecture-5e5ae796c7a9\\n- DeepSeek_Report/DeepSeek-R1.md at main · sharp119/DeepSeek_Report - GitHub : https://github.com/sharp119/DeepSeek_Report/blob/main/DeepSeek-R1.md\")]}}\n",
      "\n",
      "\n",
      "{'gather_completed_sections': {'report_sections_from_research': '\\n============================================================\\nSection 1: Technical Architecture and Capabilities\\n============================================================\\nDescription:\\nDeep dive into the model architecture, parameters, and technical specifications of DeepSeek-R1, including its reasoning abilities and performance benchmarks.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Technical Architecture and Capabilities\\n\\n**DeepSeek-R1\\'s breakthrough architecture activates only 37B of its 671B parameters per token through an innovative Mixture-of-Experts (MoE) design, dramatically improving computational efficiency while maintaining performance.**\\n\\nThe model\\'s core architecture combines three key innovations. Multi-Head Latent Attention (MLA) compresses Key-Value vectors into a latent space, reducing memory footprint by 57x compared to traditional attention mechanisms. DeepSeekMoE incorporates dynamic bias terms for load balancing without auxiliary losses, while shared experts enhance generalization across tasks.\\n\\nThe architecture includes advanced features for optimization:\\n\\n* FP8 mixed precision training that halves memory usage\\n* DualPipe parallelism for efficient communication\\n* Multi-token prediction enabling faster inference\\n* 128K context window with adaptive caching\\n\\nIn benchmarks, this architecture delivers exceptional performance, achieving 97.3% accuracy on MATH-500 and a 2,029 Elo rating on Codeforces. The model demonstrates particular strength in reasoning tasks through its novel training approach combining reinforcement learning with supervised fine-tuning stages.\\n\\n### Sources\\n- Model Architecture Behind DeepSeek R1 - Founders Creative : https://www.founderscreative.org/model-architecture-behind-deepseek-r1/\\n- DeepSeek-V3 (and R1!) Architecture | by Gal Hyams : https://medium.com/@galhyams/deepseek-v3-and-r1-architecture-5e5ae796c7a9\\n- DeepSeek_Report/DeepSeek-R1.md at main · sharp119/DeepSeek_Report - GitHub : https://github.com/sharp119/DeepSeek_Report/blob/main/DeepSeek-R1.md\\n\\n\\n============================================================\\nSection 2: DeepSeek-R1 Family and Distilled Models\\n============================================================\\nDescription:\\nOverview of the DeepSeek-R1 model variants including DeepSeek-R1-Zero and the distilled smaller models based on Llama and Qwen architectures.\\nRequires Research: \\nTrue\\n\\nContent:\\n## DeepSeek-R1 Family and Distilled Models\\n\\n**DeepSeek has created a comprehensive family of reasoning-focused models, with the flagship R1 model being distilled into smaller, more practical versions while maintaining strong reasoning capabilities.** The full DeepSeek-R1 model contains 671B parameters but only activates 37B during inference through its Mixture of Experts architecture.\\n\\nTo make this technology more accessible, DeepSeek developed six distilled models based on Qwen and Llama architectures, ranging from 1.5B to 70B parameters. These models are trained using 800,000 curated samples generated by the full R1 model.\\n\\nThe distilled models show impressive performance relative to their size. For example, the Qwen-32B distilled version achieves 72.6% accuracy on AIME 2024 mathematical reasoning tasks, approaching the full R1\\'s 79.8% while being significantly more deployable.\\n\\nThe models follow a clear performance hierarchy:\\n- Large distills (32B-70B): Near full R1 performance\\n- Medium distills (7B-14B): Strong reasoning, practical deployment\\n- Small distills (1.5B-8B): Basic reasoning capabilities, highly efficient\\n\\n### Sources\\n- Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero : https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero\\n- Reality Check: Distilled Models ≠ Full Model : https://www.groff.dev/blog/reality-check-distilled-models\\n- What are DeepSeek-R1 distilled models? : https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\\n\\n\\n============================================================\\nSection 3: Comparative Analysis: DeepSeek-R1 vs. Competitors\\n============================================================\\nDescription:\\nComparison of DeepSeek-R1 with other leading models like OpenAI\\'s o1 series, examining strengths, limitations, and unique advantages.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Comparative Analysis: DeepSeek-R1 vs. Competitors\\n\\n**DeepSeek-R1 demonstrates superior performance in mathematical and logical reasoning tasks while offering significant cost advantages through its open-source architecture, though it faces limitations in creative and general knowledge tasks.**\\n\\nKey performance metrics reveal DeepSeek-R1\\'s strengths and weaknesses compared to leading competitors:\\n\\nModel | Reasoning | General Knowledge | Cost per 1M Tokens\\n---|---|---|---\\nDeepSeek-R1 | 90.2% | 71.5% | $0.50\\nOpenAI o1 | 85.3% | 77.3% | $2.50\\nGoogle Gemini | 88.0% | 75.7% | $0.10\\n\\nIn real-world applications, DeepSeek-R1 excels in structured problem-solving. For example, in financial risk analysis, it achieved 92% accuracy in complex algebraic calculations compared to OpenAI o1\\'s 85%, demonstrating its strength in mathematical reasoning.\\n\\nHowever, DeepSeek-R1 shows limitations in creative tasks and general knowledge queries. Its responses to open-ended questions about current events often lack the contextual richness found in competitors\\' outputs. The model\\'s focus on logical reasoning comes at the cost of reduced performance in more general applications.\\n\\n### Sources\\n- DeepSeek R1 vs OpenAI O1: AI Model Comparison (2025): https://www.zignuts.com/blog/deepseek-r1-vs-openai-o1-comparison\\n- DeepSeek R1: Complete analysis of capabilities and limitations: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\\n- DeepSeek R1 Explained: Features, Benefits, and Use Cases: https://fastbots.ai/blog/deepseek-r1-explained-features-benefits-and-use-cases\\n\\n\\n============================================================\\nSection 4: Accessibility and Implementation\\n============================================================\\nDescription:\\nInformation on how to access and implement DeepSeek-R1, including API integration, deployment options, and pricing structure.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Accessibility and Implementation\\n\\n**DeepSeek-R1 offers multiple deployment options with varying hardware requirements, making it accessible through three primary methods: web interface, local installation, or API integration.** The web interface at chat.deepseek.com provides the simplest access point, while local deployment requires more technical setup but offers greater control and privacy.\\n\\nFor local installation, the model is available in different sizes to accommodate various hardware configurations. The lightweight 1.5B parameter version requires only 3.5GB RAM and an RTX 3060, while the full 671B parameter model demands substantial resources including 1,342GB RAM and multiple A100 GPUs.\\n\\nImplementation tools like Ollama and Docker simplify deployment. Ollama offers straightforward installation through command-line interface, while Docker provides containerized deployment with persistent storage. The API follows OpenAI-compatible formatting and includes intelligent caching that can reduce costs by up to 90% for repeated queries.\\n\\nPricing for API usage is structured per million tokens:\\n* Input (cache miss): $0.55\\n* Input (cache hit): $0.14\\n* Output: $2.19\\n\\n### Sources\\n- How to Run DeepSeek R1 Locally: Step-by-Step Process : https://techwiser.com/how-to-run-deepseek-r1-locally-step-by-step-process/\\n- DeepSeek R1 Review: API Pricing & How to Use DeepSeek R1 API : https://apidog.com/blog/deepseek-r1-review-api/\\n- Run DeepSeek R1 Distilled Locally (Web Interface) : https://habr.com/en/articles/881140/\\n\\n\\n============================================================\\nSection 5: DeepSeek Company Background\\n============================================================\\nDescription:\\nBrief history of DeepSeek as a company, its founding, development trajectory, and position in the AI industry landscape.\\nRequires Research: \\nTrue\\n\\nContent:\\n## DeepSeek Company Background\\n\\n**DeepSeek represents a new model of AI company development, emerging from High-Flyer\\'s quantitative trading division to become a standalone AI powerhouse that achieved breakthrough capabilities with remarkably efficient resource utilization**. Founded in May 2023 by Liang Wenfeng, the company has demonstrated that massive computing infrastructure and multibillion-dollar budgets aren\\'t prerequisites for AI innovation.\\n\\nDeepSeek\\'s development trajectory includes several key milestones:\\n\\n- 2019: Initial AI research division established within High-Flyer\\n- 2023: Spin-off as independent entity\\n- 2024: Release of DeepSeek-V2 and V3 models\\n- 2025: Consideration of first external funding round\\n\\nThe company gained prominence for developing high-performance AI models at a fraction of competitors\\' costs. Most notably, their DeepSeek-V3 model, developed for approximately $5.58 million, achieved performance comparable to industry leaders while using significantly fewer resources.\\n\\nCurrently, DeepSeek maintains a research-focused approach with limited commercialization. Primarily funded by High-Flyer, the company has earned recognition as the \"Pinduoduo of AI\" for its cost-efficient development strategy that challenges conventional assumptions about necessary AI investment scales.\\n\\n### Sources\\n- DeepSeek has accelerated the race for global AI dominance: https://www.businessthink.unsw.edu.au/articles/deepseek-china-ai-industry-development\\n- DeepSeek sparks debate over Big Tech\\'s investment strategies: https://www.usatoday.com/story/money/2025/01/29/deepseek-big-tech-overspending-investments/78028924007/\\n- Industry Leaders React to DeepSeek\\'s Impact on the Market: https://futureweek.com/industry-leaders-react-to-deepseek/\\n\\n'}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Introduction to DeepSeek-R1', description='Brief overview of DeepSeek-R1, its significance in the AI landscape, and the main capabilities that distinguish it as a reasoning-focused language model.', research=False, content=\"# Introduction to DeepSeek-R1\\n\\nDeepSeek-R1 represents a significant advancement in AI language models, particularly in mathematical and logical reasoning capabilities. Through its innovative Mixture-of-Experts architecture, the model achieves remarkable efficiency by activating only 37B of its 671B parameters during inference, dramatically reducing computational costs while maintaining high performance.\\n\\nThe model's development by DeepSeek, emerging from High-Flyer's quantitative trading division, demonstrates that breakthrough AI capabilities can be achieved without massive computing infrastructure or multibillion-dollar budgets. This report examines DeepSeek-R1's technical architecture, model variants, comparative performance, and implementation options, providing a comprehensive analysis of its impact on the AI landscape.\")]}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Conclusion: Impact and Future Prospects', description=\"Summary of DeepSeek-R1's significance, current impact on the AI field, and potential future developments, with a structured comparison table of key features.\", research=False, content=\"## Conclusion: Impact and Future Prospects\\n\\nDeepSeek-R1 represents a significant advancement in efficient AI architecture, demonstrating that sophisticated reasoning capabilities can be achieved through innovative design rather than raw computational power. Its MoE architecture and ability to activate only 37B of 671B parameters has established new benchmarks for model efficiency while maintaining competitive performance.\\n\\nFeature | DeepSeek-R1 | Traditional Models\\n---|---|---\\nParameter Efficiency | 37B active/671B total | All parameters active\\nMemory Footprint | 57x reduction | Baseline\\nContext Window | 128K tokens | 32K-64K typical\\nCost per 1M Tokens | $0.50 | $1.50-2.50\\nMathematical Reasoning | 97.3% accuracy | 85-90% accuracy\\n\\nLooking ahead, DeepSeek-R1's architecture may influence the broader AI industry's approach to model design, potentially shifting focus from parameter count to activation efficiency. The success of its distilled models suggests a future where sophisticated AI capabilities become increasingly accessible to smaller organizations and developers, though continued development is needed to address current limitations in creative and general knowledge tasks.\")]}}\n",
      "\n",
      "\n",
      "{'compile_final_report': {'final_report': '# Introduction to DeepSeek-R1\\n\\nDeepSeek-R1 represents a significant advancement in AI language models, particularly in mathematical and logical reasoning capabilities. Through its innovative Mixture-of-Experts architecture, the model achieves remarkable efficiency by activating only 37B of its 671B parameters during inference, dramatically reducing computational costs while maintaining high performance.\\n\\nThe model\\'s development by DeepSeek, emerging from High-Flyer\\'s quantitative trading division, demonstrates that breakthrough AI capabilities can be achieved without massive computing infrastructure or multibillion-dollar budgets. This report examines DeepSeek-R1\\'s technical architecture, model variants, comparative performance, and implementation options, providing a comprehensive analysis of its impact on the AI landscape.\\n\\n## Technical Architecture and Capabilities\\n\\n**DeepSeek-R1\\'s breakthrough architecture activates only 37B of its 671B parameters per token through an innovative Mixture-of-Experts (MoE) design, dramatically improving computational efficiency while maintaining performance.**\\n\\nThe model\\'s core architecture combines three key innovations. Multi-Head Latent Attention (MLA) compresses Key-Value vectors into a latent space, reducing memory footprint by 57x compared to traditional attention mechanisms. DeepSeekMoE incorporates dynamic bias terms for load balancing without auxiliary losses, while shared experts enhance generalization across tasks.\\n\\nThe architecture includes advanced features for optimization:\\n\\n* FP8 mixed precision training that halves memory usage\\n* DualPipe parallelism for efficient communication\\n* Multi-token prediction enabling faster inference\\n* 128K context window with adaptive caching\\n\\nIn benchmarks, this architecture delivers exceptional performance, achieving 97.3% accuracy on MATH-500 and a 2,029 Elo rating on Codeforces. The model demonstrates particular strength in reasoning tasks through its novel training approach combining reinforcement learning with supervised fine-tuning stages.\\n\\n### Sources\\n- Model Architecture Behind DeepSeek R1 - Founders Creative : https://www.founderscreative.org/model-architecture-behind-deepseek-r1/\\n- DeepSeek-V3 (and R1!) Architecture | by Gal Hyams : https://medium.com/@galhyams/deepseek-v3-and-r1-architecture-5e5ae796c7a9\\n- DeepSeek_Report/DeepSeek-R1.md at main · sharp119/DeepSeek_Report - GitHub : https://github.com/sharp119/DeepSeek_Report/blob/main/DeepSeek-R1.md\\n\\n## DeepSeek-R1 Family and Distilled Models\\n\\n**DeepSeek has created a comprehensive family of reasoning-focused models, with the flagship R1 model being distilled into smaller, more practical versions while maintaining strong reasoning capabilities.** The full DeepSeek-R1 model contains 671B parameters but only activates 37B during inference through its Mixture of Experts architecture.\\n\\nTo make this technology more accessible, DeepSeek developed six distilled models based on Qwen and Llama architectures, ranging from 1.5B to 70B parameters. These models are trained using 800,000 curated samples generated by the full R1 model.\\n\\nThe distilled models show impressive performance relative to their size. For example, the Qwen-32B distilled version achieves 72.6% accuracy on AIME 2024 mathematical reasoning tasks, approaching the full R1\\'s 79.8% while being significantly more deployable.\\n\\nThe models follow a clear performance hierarchy:\\n- Large distills (32B-70B): Near full R1 performance\\n- Medium distills (7B-14B): Strong reasoning, practical deployment\\n- Small distills (1.5B-8B): Basic reasoning capabilities, highly efficient\\n\\n### Sources\\n- Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero : https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero\\n- Reality Check: Distilled Models ≠ Full Model : https://www.groff.dev/blog/reality-check-distilled-models\\n- What are DeepSeek-R1 distilled models? : https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\\n\\n## Comparative Analysis: DeepSeek-R1 vs. Competitors\\n\\n**DeepSeek-R1 demonstrates superior performance in mathematical and logical reasoning tasks while offering significant cost advantages through its open-source architecture, though it faces limitations in creative and general knowledge tasks.**\\n\\nKey performance metrics reveal DeepSeek-R1\\'s strengths and weaknesses compared to leading competitors:\\n\\nModel | Reasoning | General Knowledge | Cost per 1M Tokens\\n---|---|---|---\\nDeepSeek-R1 | 90.2% | 71.5% | $0.50\\nOpenAI o1 | 85.3% | 77.3% | $2.50\\nGoogle Gemini | 88.0% | 75.7% | $0.10\\n\\nIn real-world applications, DeepSeek-R1 excels in structured problem-solving. For example, in financial risk analysis, it achieved 92% accuracy in complex algebraic calculations compared to OpenAI o1\\'s 85%, demonstrating its strength in mathematical reasoning.\\n\\nHowever, DeepSeek-R1 shows limitations in creative tasks and general knowledge queries. Its responses to open-ended questions about current events often lack the contextual richness found in competitors\\' outputs. The model\\'s focus on logical reasoning comes at the cost of reduced performance in more general applications.\\n\\n### Sources\\n- DeepSeek R1 vs OpenAI O1: AI Model Comparison (2025): https://www.zignuts.com/blog/deepseek-r1-vs-openai-o1-comparison\\n- DeepSeek R1: Complete analysis of capabilities and limitations: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\\n- DeepSeek R1 Explained: Features, Benefits, and Use Cases: https://fastbots.ai/blog/deepseek-r1-explained-features-benefits-and-use-cases\\n\\n## Accessibility and Implementation\\n\\n**DeepSeek-R1 offers multiple deployment options with varying hardware requirements, making it accessible through three primary methods: web interface, local installation, or API integration.** The web interface at chat.deepseek.com provides the simplest access point, while local deployment requires more technical setup but offers greater control and privacy.\\n\\nFor local installation, the model is available in different sizes to accommodate various hardware configurations. The lightweight 1.5B parameter version requires only 3.5GB RAM and an RTX 3060, while the full 671B parameter model demands substantial resources including 1,342GB RAM and multiple A100 GPUs.\\n\\nImplementation tools like Ollama and Docker simplify deployment. Ollama offers straightforward installation through command-line interface, while Docker provides containerized deployment with persistent storage. The API follows OpenAI-compatible formatting and includes intelligent caching that can reduce costs by up to 90% for repeated queries.\\n\\nPricing for API usage is structured per million tokens:\\n* Input (cache miss): $0.55\\n* Input (cache hit): $0.14\\n* Output: $2.19\\n\\n### Sources\\n- How to Run DeepSeek R1 Locally: Step-by-Step Process : https://techwiser.com/how-to-run-deepseek-r1-locally-step-by-step-process/\\n- DeepSeek R1 Review: API Pricing & How to Use DeepSeek R1 API : https://apidog.com/blog/deepseek-r1-review-api/\\n- Run DeepSeek R1 Distilled Locally (Web Interface) : https://habr.com/en/articles/881140/\\n\\n## DeepSeek Company Background\\n\\n**DeepSeek represents a new model of AI company development, emerging from High-Flyer\\'s quantitative trading division to become a standalone AI powerhouse that achieved breakthrough capabilities with remarkably efficient resource utilization**. Founded in May 2023 by Liang Wenfeng, the company has demonstrated that massive computing infrastructure and multibillion-dollar budgets aren\\'t prerequisites for AI innovation.\\n\\nDeepSeek\\'s development trajectory includes several key milestones:\\n\\n- 2019: Initial AI research division established within High-Flyer\\n- 2023: Spin-off as independent entity\\n- 2024: Release of DeepSeek-V2 and V3 models\\n- 2025: Consideration of first external funding round\\n\\nThe company gained prominence for developing high-performance AI models at a fraction of competitors\\' costs. Most notably, their DeepSeek-V3 model, developed for approximately $5.58 million, achieved performance comparable to industry leaders while using significantly fewer resources.\\n\\nCurrently, DeepSeek maintains a research-focused approach with limited commercialization. Primarily funded by High-Flyer, the company has earned recognition as the \"Pinduoduo of AI\" for its cost-efficient development strategy that challenges conventional assumptions about necessary AI investment scales.\\n\\n### Sources\\n- DeepSeek has accelerated the race for global AI dominance: https://www.businessthink.unsw.edu.au/articles/deepseek-china-ai-industry-development\\n- DeepSeek sparks debate over Big Tech\\'s investment strategies: https://www.usatoday.com/story/money/2025/01/29/deepseek-big-tech-overspending-investments/78028924007/\\n- Industry Leaders React to DeepSeek\\'s Impact on the Market: https://futureweek.com/industry-leaders-react-to-deepseek/\\n\\n## Conclusion: Impact and Future Prospects\\n\\nDeepSeek-R1 represents a significant advancement in efficient AI architecture, demonstrating that sophisticated reasoning capabilities can be achieved through innovative design rather than raw computational power. Its MoE architecture and ability to activate only 37B of 671B parameters has established new benchmarks for model efficiency while maintaining competitive performance.\\n\\nFeature | DeepSeek-R1 | Traditional Models\\n---|---|---\\nParameter Efficiency | 37B active/671B total | All parameters active\\nMemory Footprint | 57x reduction | Baseline\\nContext Window | 128K tokens | 32K-64K typical\\nCost per 1M Tokens | $0.50 | $1.50-2.50\\nMathematical Reasoning | 97.3% accuracy | 85-90% accuracy\\n\\nLooking ahead, DeepSeek-R1\\'s architecture may influence the broader AI industry\\'s approach to model design, potentially shifting focus from parameter count to activation efficiency. The success of its distilled models suggests a future where sophisticated AI capabilities become increasingly accessible to smaller organizations and developers, though continued development is needed to address current limitations in creative and general knowledge tasks.'}}\n",
      "\n",
      "\n",
      "🎉 Final report generated! 🎉\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# DeepSeek-R1 Report\n",
       "\n",
       "# Introduction to DeepSeek-R1\n",
       "\n",
       "DeepSeek-R1 represents a significant advancement in AI language models, particularly in mathematical and logical reasoning capabilities. Through its innovative Mixture-of-Experts architecture, the model achieves remarkable efficiency by activating only 37B of its 671B parameters during inference, dramatically reducing computational costs while maintaining high performance.\n",
       "\n",
       "The model's development by DeepSeek, emerging from High-Flyer's quantitative trading division, demonstrates that breakthrough AI capabilities can be achieved without massive computing infrastructure or multibillion-dollar budgets. This report examines DeepSeek-R1's technical architecture, model variants, comparative performance, and implementation options, providing a comprehensive analysis of its impact on the AI landscape.\n",
       "\n",
       "## Technical Architecture and Capabilities\n",
       "\n",
       "**DeepSeek-R1's breakthrough architecture activates only 37B of its 671B parameters per token through an innovative Mixture-of-Experts (MoE) design, dramatically improving computational efficiency while maintaining performance.**\n",
       "\n",
       "The model's core architecture combines three key innovations. Multi-Head Latent Attention (MLA) compresses Key-Value vectors into a latent space, reducing memory footprint by 57x compared to traditional attention mechanisms. DeepSeekMoE incorporates dynamic bias terms for load balancing without auxiliary losses, while shared experts enhance generalization across tasks.\n",
       "\n",
       "The architecture includes advanced features for optimization:\n",
       "\n",
       "* FP8 mixed precision training that halves memory usage\n",
       "* DualPipe parallelism for efficient communication\n",
       "* Multi-token prediction enabling faster inference\n",
       "* 128K context window with adaptive caching\n",
       "\n",
       "In benchmarks, this architecture delivers exceptional performance, achieving 97.3% accuracy on MATH-500 and a 2,029 Elo rating on Codeforces. The model demonstrates particular strength in reasoning tasks through its novel training approach combining reinforcement learning with supervised fine-tuning stages.\n",
       "\n",
       "### Sources\n",
       "- Model Architecture Behind DeepSeek R1 - Founders Creative : https://www.founderscreative.org/model-architecture-behind-deepseek-r1/\n",
       "- DeepSeek-V3 (and R1!) Architecture | by Gal Hyams : https://medium.com/@galhyams/deepseek-v3-and-r1-architecture-5e5ae796c7a9\n",
       "- DeepSeek_Report/DeepSeek-R1.md at main · sharp119/DeepSeek_Report - GitHub : https://github.com/sharp119/DeepSeek_Report/blob/main/DeepSeek-R1.md\n",
       "\n",
       "## DeepSeek-R1 Family and Distilled Models\n",
       "\n",
       "**DeepSeek has created a comprehensive family of reasoning-focused models, with the flagship R1 model being distilled into smaller, more practical versions while maintaining strong reasoning capabilities.** The full DeepSeek-R1 model contains 671B parameters but only activates 37B during inference through its Mixture of Experts architecture.\n",
       "\n",
       "To make this technology more accessible, DeepSeek developed six distilled models based on Qwen and Llama architectures, ranging from 1.5B to 70B parameters. These models are trained using 800,000 curated samples generated by the full R1 model.\n",
       "\n",
       "The distilled models show impressive performance relative to their size. For example, the Qwen-32B distilled version achieves 72.6% accuracy on AIME 2024 mathematical reasoning tasks, approaching the full R1's 79.8% while being significantly more deployable.\n",
       "\n",
       "The models follow a clear performance hierarchy:\n",
       "- Large distills (32B-70B): Near full R1 performance\n",
       "- Medium distills (7B-14B): Strong reasoning, practical deployment\n",
       "- Small distills (1.5B-8B): Basic reasoning capabilities, highly efficient\n",
       "\n",
       "### Sources\n",
       "- Ultimate Comparison of DeepSeek Models: V3, R1, and R1-Zero : https://blog.spheron.network/ultimate-comparison-of-deepseek-models-v3-r1-and-r1-zero\n",
       "- Reality Check: Distilled Models ≠ Full Model : https://www.groff.dev/blog/reality-check-distilled-models\n",
       "- What are DeepSeek-R1 distilled models? : https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d\n",
       "\n",
       "## Comparative Analysis: DeepSeek-R1 vs. Competitors\n",
       "\n",
       "**DeepSeek-R1 demonstrates superior performance in mathematical and logical reasoning tasks while offering significant cost advantages through its open-source architecture, though it faces limitations in creative and general knowledge tasks.**\n",
       "\n",
       "Key performance metrics reveal DeepSeek-R1's strengths and weaknesses compared to leading competitors:\n",
       "\n",
       "Model | Reasoning | General Knowledge | Cost per 1M Tokens\n",
       "---|---|---|---\n",
       "DeepSeek-R1 | 90.2% | 71.5% | $0.50\n",
       "OpenAI o1 | 85.3% | 77.3% | $2.50\n",
       "Google Gemini | 88.0% | 75.7% | $0.10\n",
       "\n",
       "In real-world applications, DeepSeek-R1 excels in structured problem-solving. For example, in financial risk analysis, it achieved 92% accuracy in complex algebraic calculations compared to OpenAI o1's 85%, demonstrating its strength in mathematical reasoning.\n",
       "\n",
       "However, DeepSeek-R1 shows limitations in creative tasks and general knowledge queries. Its responses to open-ended questions about current events often lack the contextual richness found in competitors' outputs. The model's focus on logical reasoning comes at the cost of reduced performance in more general applications.\n",
       "\n",
       "### Sources\n",
       "- DeepSeek R1 vs OpenAI O1: AI Model Comparison (2025): https://www.zignuts.com/blog/deepseek-r1-vs-openai-o1-comparison\n",
       "- DeepSeek R1: Complete analysis of capabilities and limitations: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\n",
       "- DeepSeek R1 Explained: Features, Benefits, and Use Cases: https://fastbots.ai/blog/deepseek-r1-explained-features-benefits-and-use-cases\n",
       "\n",
       "## Accessibility and Implementation\n",
       "\n",
       "**DeepSeek-R1 offers multiple deployment options with varying hardware requirements, making it accessible through three primary methods: web interface, local installation, or API integration.** The web interface at chat.deepseek.com provides the simplest access point, while local deployment requires more technical setup but offers greater control and privacy.\n",
       "\n",
       "For local installation, the model is available in different sizes to accommodate various hardware configurations. The lightweight 1.5B parameter version requires only 3.5GB RAM and an RTX 3060, while the full 671B parameter model demands substantial resources including 1,342GB RAM and multiple A100 GPUs.\n",
       "\n",
       "Implementation tools like Ollama and Docker simplify deployment. Ollama offers straightforward installation through command-line interface, while Docker provides containerized deployment with persistent storage. The API follows OpenAI-compatible formatting and includes intelligent caching that can reduce costs by up to 90% for repeated queries.\n",
       "\n",
       "Pricing for API usage is structured per million tokens:\n",
       "* Input (cache miss): $0.55\n",
       "* Input (cache hit): $0.14\n",
       "* Output: $2.19\n",
       "\n",
       "### Sources\n",
       "- How to Run DeepSeek R1 Locally: Step-by-Step Process : https://techwiser.com/how-to-run-deepseek-r1-locally-step-by-step-process/\n",
       "- DeepSeek R1 Review: API Pricing & How to Use DeepSeek R1 API : https://apidog.com/blog/deepseek-r1-review-api/\n",
       "- Run DeepSeek R1 Distilled Locally (Web Interface) : https://habr.com/en/articles/881140/\n",
       "\n",
       "## DeepSeek Company Background\n",
       "\n",
       "**DeepSeek represents a new model of AI company development, emerging from High-Flyer's quantitative trading division to become a standalone AI powerhouse that achieved breakthrough capabilities with remarkably efficient resource utilization**. Founded in May 2023 by Liang Wenfeng, the company has demonstrated that massive computing infrastructure and multibillion-dollar budgets aren't prerequisites for AI innovation.\n",
       "\n",
       "DeepSeek's development trajectory includes several key milestones:\n",
       "\n",
       "- 2019: Initial AI research division established within High-Flyer\n",
       "- 2023: Spin-off as independent entity\n",
       "- 2024: Release of DeepSeek-V2 and V3 models\n",
       "- 2025: Consideration of first external funding round\n",
       "\n",
       "The company gained prominence for developing high-performance AI models at a fraction of competitors' costs. Most notably, their DeepSeek-V3 model, developed for approximately $5.58 million, achieved performance comparable to industry leaders while using significantly fewer resources.\n",
       "\n",
       "Currently, DeepSeek maintains a research-focused approach with limited commercialization. Primarily funded by High-Flyer, the company has earned recognition as the \"Pinduoduo of AI\" for its cost-efficient development strategy that challenges conventional assumptions about necessary AI investment scales.\n",
       "\n",
       "### Sources\n",
       "- DeepSeek has accelerated the race for global AI dominance: https://www.businessthink.unsw.edu.au/articles/deepseek-china-ai-industry-development\n",
       "- DeepSeek sparks debate over Big Tech's investment strategies: https://www.usatoday.com/story/money/2025/01/29/deepseek-big-tech-overspending-investments/78028924007/\n",
       "- Industry Leaders React to DeepSeek's Impact on the Market: https://futureweek.com/industry-leaders-react-to-deepseek/\n",
       "\n",
       "## Conclusion: Impact and Future Prospects\n",
       "\n",
       "DeepSeek-R1 represents a significant advancement in efficient AI architecture, demonstrating that sophisticated reasoning capabilities can be achieved through innovative design rather than raw computational power. Its MoE architecture and ability to activate only 37B of 671B parameters has established new benchmarks for model efficiency while maintaining competitive performance.\n",
       "\n",
       "Feature | DeepSeek-R1 | Traditional Models\n",
       "---|---|---\n",
       "Parameter Efficiency | 37B active/671B total | All parameters active\n",
       "Memory Footprint | 57x reduction | Baseline\n",
       "Context Window | 128K tokens | 32K-64K typical\n",
       "Cost per 1M Tokens | $0.50 | $1.50-2.50\n",
       "Mathematical Reasoning | 97.3% accuracy | 85-90% accuracy\n",
       "\n",
       "Looking ahead, DeepSeek-R1's architecture may influence the broader AI industry's approach to model design, potentially shifting focus from parameter count to activation efficiency. The success of its distilled models suggests a future where sophisticated AI capabilities become increasingly accessible to smaller organizations and developers, though continued development is needed to address current limitations in creative and general knowledge tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await approve_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
